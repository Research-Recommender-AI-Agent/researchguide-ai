{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0927ab4a-1db6-420d-be9e-98d1e9193f70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setting(오류 방지를 위해 실행 권장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ad4291e-17a0-4e3b-9090-e06707ca1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Run this ONCE at the very top of your notebook ===\n",
    "# import os, warnings\n",
    "\n",
    "# # transformers/torch의 각종 경고 잠재우기\n",
    "# os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"   # HF 권고/Deprecation 메시지 억제\n",
    "# os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"        # 진행바 억제(선택)\n",
    "# os.environ[\"PYTHONWARNINGS\"] = \"ignore::FutureWarning\"   # 파이썬 FutureWarning 전역 무시\n",
    "# os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# # 특정 메시지만 골라 무시하고 싶다면(더 보수적)\n",
    "# warnings.filterwarnings(\n",
    "#     \"ignore\",\n",
    "#     category=FutureWarning,\n",
    "#     message=r\"`encoder_attention_mask` is deprecated and will be removed in version 4\\.55\\.0\",\n",
    "# )\n",
    "\n",
    "# # HF 로깅 레벨 낮추기(선택)\n",
    "# try:\n",
    "#     from transformers.utils import logging as hf_logging\n",
    "#     hf_logging.set_verbosity_error()  # or set_verbosity_warning()\n",
    "# except Exception:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf3f4b16-10a4-4fd1-981d-c49207e9d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import CrossEncoder\n",
    "# import torch\n",
    "\n",
    "# _ce_model_cache = None\n",
    "# def ce_predict_pairs(pairs):\n",
    "#     global _ce_model_cache\n",
    "#     if not USE_CE:\n",
    "#         return np.zeros(len(pairs), dtype=float)\n",
    "\n",
    "#     dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     if _ce_model_cache is None:\n",
    "#         _ce_model_cache = CrossEncoder(\n",
    "#             CE_MODEL,\n",
    "#             device=dev,\n",
    "#             max_length=512,\n",
    "#             trust_remote_code=True,                 # Jina CE는 custom code 필요\n",
    "#             model_kwargs={\"attn_implementation\":\"sdpa\"}  # ★ SDPA 사용\n",
    "#         )\n",
    "#     return _ce_model_cache.predict(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10f7c868-4b70-463a-93fd-555e9137d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (0.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc1ca4b-0a26-45d0-9160-cf4071b960c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-win_amd64.whl.metadata (4.7 kB)\n",
      "Downloading hf_xet-1.1.10-cp37-abi3-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.3/2.8 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.6/2.8 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 6.0 MB/s  0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.1.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6a96148-3432-44cb-b5c7-f585fc7e9871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\ogod3\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from accelerate) (2.8.0+cu129)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from accelerate) (0.34.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.7.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ogod3\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.7.14)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.10.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts accelerate-config.exe, accelerate-estimate-memory.exe, accelerate-launch.exe, accelerate-merge-weights.exe and accelerate.exe are installed in 'C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install -U accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29d1ca-492d-4883-a499-23d54bf01be0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665116a6-56d9-41d0-b39d-83873e53dc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>keywords</th>\n",
       "      <th>org</th>\n",
       "      <th>year</th>\n",
       "      <th>url</th>\n",
       "      <th>doi</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>de776da1a1d0f302eeb7608c32c810d3</td>\n",
       "      <td>This item has been removed from publication</td>\n",
       "      <td>En este archivo se encuentran los resultados d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>OpenAIRE</td>\n",
       "      <td>9999</td>\n",
       "      <td>https://dx.doi.org/10.71590/rsfwiq/0yfh7w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18fce9b918b6abc277517b4a065df9e5</td>\n",
       "      <td>Homemade Toys</td>\n",
       "      <td>Supported by funding from the Department of Ar...</td>\n",
       "      <td>['Toys;Bréagáin;An Baile Dubh;Ballyduff']</td>\n",
       "      <td>OpenAIRE</td>\n",
       "      <td>7938</td>\n",
       "      <td>https://dx.doi.org/10.7925/drs1.duchas_4504414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a8331499d9a5e7796fdfa528657b7143</td>\n",
       "      <td>Local Forge</td>\n",
       "      <td>Supported by funding from the Department of Ar...</td>\n",
       "      <td>['Smithing;Gaibhneacht']</td>\n",
       "      <td>OpenAIRE</td>\n",
       "      <td>4938</td>\n",
       "      <td>https://dx.doi.org/10.7925/drs1.duchas_4927495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>417bb310b9aeb60a449b9a4765b3e2e8</td>\n",
       "      <td>Homemade Toys</td>\n",
       "      <td>Supported by funding from the Department of Ar...</td>\n",
       "      <td>['Tyrrellspass;Bealach an Tirialaigh;Toys;Bréa...</td>\n",
       "      <td>OpenAIRE</td>\n",
       "      <td>2938</td>\n",
       "      <td>https://dx.doi.org/10.7925/drs1.duchas_5119780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e81bb07a6b450ac61e926e8b27d0958e</td>\n",
       "      <td>Agta hunter-gatherer oral microbiomes are shap...</td>\n",
       "      <td>Here we investigate the effects of extensive s...</td>\n",
       "      <td>['Hunter-gatherers;Oral microbiome;Disease spr...</td>\n",
       "      <td>OpenAIRE</td>\n",
       "      <td>2030</td>\n",
       "      <td>https://dx.doi.org/10.5281/zenodo.6338839;http...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  de776da1a1d0f302eeb7608c32c810d3   \n",
       "1  18fce9b918b6abc277517b4a065df9e5   \n",
       "2  a8331499d9a5e7796fdfa528657b7143   \n",
       "3  417bb310b9aeb60a449b9a4765b3e2e8   \n",
       "4  e81bb07a6b450ac61e926e8b27d0958e   \n",
       "\n",
       "                                               title  \\\n",
       "0        This item has been removed from publication   \n",
       "1                                      Homemade Toys   \n",
       "2                                        Local Forge   \n",
       "3                                      Homemade Toys   \n",
       "4  Agta hunter-gatherer oral microbiomes are shap...   \n",
       "\n",
       "                                         description  \\\n",
       "0  En este archivo se encuentran los resultados d...   \n",
       "1  Supported by funding from the Department of Ar...   \n",
       "2  Supported by funding from the Department of Ar...   \n",
       "3  Supported by funding from the Department of Ar...   \n",
       "4  Here we investigate the effects of extensive s...   \n",
       "\n",
       "                                            keywords       org  year  \\\n",
       "0                                                 []  OpenAIRE  9999   \n",
       "1          ['Toys;Bréagáin;An Baile Dubh;Ballyduff']  OpenAIRE  7938   \n",
       "2                           ['Smithing;Gaibhneacht']  OpenAIRE  4938   \n",
       "3  ['Tyrrellspass;Bealach an Tirialaigh;Toys;Bréa...  OpenAIRE  2938   \n",
       "4  ['Hunter-gatherers;Oral microbiome;Disease spr...  OpenAIRE  2030   \n",
       "\n",
       "                                                 url  doi lang  \n",
       "0          https://dx.doi.org/10.71590/rsfwiq/0yfh7w  NaN   es  \n",
       "1     https://dx.doi.org/10.7925/drs1.duchas_4504414  NaN   en  \n",
       "2     https://dx.doi.org/10.7925/drs1.duchas_4927495  NaN   en  \n",
       "3     https://dx.doi.org/10.7925/drs1.duchas_5119780  NaN   en  \n",
       "4  https://dx.doi.org/10.5281/zenodo.6338839;http...  NaN   en  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clean = pd.read_csv(\"datasets_clean.csv\",encoding='utf-8-sig')\n",
    "clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8479f7ef-314f-4ef7-9bff-46fcbf980cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "en      2391\n",
       "ko         8\n",
       "tl         2\n",
       "es         1\n",
       "ca         1\n",
       "et         1\n",
       "pt         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.value_counts(['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae6737e-68e7-415e-b313-fefec79b5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df: pandas DataFrame\n",
    "mask = clean['lang'].astype(str).str.strip().str.lower().eq('en')\n",
    "clean = clean.loc[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9d8d34-25e4-4744-86a8-a820de3f4677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang\n",
       "en      2391\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.value_counts(['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c1e819-1152-4ad0-bcce-0509a8ebd3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>reason</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>코로나 대전된 PVDF 필름의 열자격 전류</td>\n",
       "      <td>본 논문에서는 두께 50[㎛]의 미연신 α형 폴리비닐덴 후로라이드 [Polyviny...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>코로나 대전된 루미롤 필름의 열자격 전류 특성에 관한 연구</td>\n",
       "      <td>온도범위 293~403[˚K]에서 코로나 대전된 루미롤 필름으로부터 열자격전류를 측...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>동물세포에 결합하는 사스 코로나 바이러스 RBD에 관한 연구</td>\n",
       "      <td>사스(Severe Acute Respiratory Syndrome) 코로나 바이러스...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>코로나 바이러스 감염증-19 (COVID-19)가 스포츠 활동에 미치는 영향에 대한...</td>\n",
       "      <td>본 연구는 코로나 바이러스 감염증-19 (COVID-19, 이하 코로나 19)가 스...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fc 결합이 중동호흡기증후군 코로나바이러스 스파이크 단백질의 면역원성에 미치는 영향</td>\n",
       "      <td>중동호흡기증후군 코로나 바이러스는 인간에게 감염되었을 시 사망률이 약 ~36%로 보...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                            코로나 대전된 PVDF 필름의 열자격 전류   \n",
       "1                   코로나 대전된 루미롤 필름의 열자격 전류 특성에 관한 연구   \n",
       "2                  동물세포에 결합하는 사스 코로나 바이러스 RBD에 관한 연구   \n",
       "3  코로나 바이러스 감염증-19 (COVID-19)가 스포츠 활동에 미치는 영향에 대한...   \n",
       "4     Fc 결합이 중동호흡기증후군 코로나바이러스 스파이크 단백질의 면역원성에 미치는 영향   \n",
       "\n",
       "                                         description  \\\n",
       "0  본 논문에서는 두께 50[㎛]의 미연신 α형 폴리비닐덴 후로라이드 [Polyviny...   \n",
       "1  온도범위 293~403[˚K]에서 코로나 대전된 루미롤 필름으로부터 열자격전류를 측...   \n",
       "2  사스(Severe Acute Respiratory Syndrome) 코로나 바이러스...   \n",
       "3  본 연구는 코로나 바이러스 감염증-19 (COVID-19, 이하 코로나 19)가 스...   \n",
       "4  중동호흡기증후군 코로나 바이러스는 인간에게 감염되었을 시 사망률이 약 ~36%로 보...   \n",
       "\n",
       "                                                 url  score  reason  level  \n",
       "0  http://click.ndsl.kr/servlet/OpenAPIDetailView...    NaN     NaN    NaN  \n",
       "1  http://click.ndsl.kr/servlet/OpenAPIDetailView...    NaN     NaN    NaN  \n",
       "2  http://click.ndsl.kr/servlet/OpenAPIDetailView...    NaN     NaN    NaN  \n",
       "3  http://click.ndsl.kr/servlet/OpenAPIDetailView...    NaN     NaN    NaN  \n",
       "4  http://click.ndsl.kr/servlet/OpenAPIDetailView...    NaN     NaN    NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper = pd.read_csv(csv_file)\n",
    "paper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ba9eb7-72fa-4e39-a40a-95d6a66d5168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            0\n",
       "description     29\n",
       "url              0\n",
       "score          400\n",
       "reason         400\n",
       "level          400\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42ae82-2e7a-4c9f-827f-5b6d4d2554d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 전처리(결측치 삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "713351e7-bbcf-493a-b6f8-9f1b2148d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 전처리 완료: 33행 제거 → 367행 남김\n",
      "저장 위치: C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\papers_clean.prep.csv\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# 전처리 1단계: 결측치 제거 + 중복 제거\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 입력/출력 경로\n",
    "SRC = \"papers_clean.csv\"\n",
    "DST = \"papers_clean.prep.csv\"\n",
    "\n",
    "# 1) 로드\n",
    "df = pd.read_csv(SRC)\n",
    "\n",
    "# 2) 컬럼명 소문자로 통일(혼용 대비)\n",
    "df = df.rename(columns={c: c.lower() for c in df.columns})\n",
    "\n",
    "# 3) 결측치로 간주할 문자열 패턴들 정의\n",
    "NULL_MARKERS = {\"\", \" \", \"nan\", \"none\", \"null\", \"-\", \"--\"}\n",
    "def normalize_nulls(s):\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s2 = str(s).strip()\n",
    "    return np.nan if s2.lower() in NULL_MARKERS or len(s2) == 0 else s2\n",
    "\n",
    "# 4) title/description/url 기본 정리(공백 정리 + NULL_MARKERS → NaN)\n",
    "for col in [\"title\", \"description\", \"url\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].map(normalize_nulls)\n",
    "    else:\n",
    "        # 없으면 만들어 두되 NaN으로 처리\n",
    "        df[col] = np.nan\n",
    "\n",
    "before = len(df)\n",
    "\n",
    "# 5) description 결측 행 제거\n",
    "df = df.dropna(subset=[\"description\"])\n",
    "\n",
    "# 6) 중복 행 제거 (title+description 기준)\n",
    "# df = df.drop_duplicates(subset=[\"title\", \"description\"])\n",
    "df = df.drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "# (선택) title도 비어있으면 제거\n",
    "# df = df.dropna(subset=[\"title\"])\n",
    "\n",
    "after = len(df)\n",
    "removed = before - after\n",
    "\n",
    "# 7) 저장\n",
    "df.to_csv(DST, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\" 전처리 완료: {removed}행 제거 → {after}행 남김\")\n",
    "print(f\"저장 위치: {os.path.abspath(DST)}\")\n",
    "# 필요시 미리보기\n",
    "# display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d0c34d3-64eb-4f6a-98c2-b9b69aaeace7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 전처리 완료: 745행 제거 → 1660행 남김\n",
      "저장 위치: C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\datasets_clean_prep.csv\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# 전처리 1단계: 결측치 제거 + 중복 제거\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 입력/출력 경로\n",
    "SRC1 = \"datasets_clean.csv\"\n",
    "DST1 = \"datasets_clean_prep.csv\"\n",
    "\n",
    "# 1) 로드\n",
    "df1 = pd.read_csv(SRC1)\n",
    "\n",
    "# 2) 컬럼명 소문자로 통일(혼용 대비)\n",
    "df1 = df1.rename(columns={c: c.lower() for c in df1.columns})\n",
    "\n",
    "# 3) 결측치로 간주할 문자열 패턴들 정의\n",
    "NULL_MARKERS = {\"\", \" \", \"nan\", \"none\", \"null\", \"-\", \"--\"}\n",
    "def normalize_nulls(s):\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s2 = str(s).strip()\n",
    "    return np.nan if s2.lower() in NULL_MARKERS or len(s2) == 0 else s2\n",
    "\n",
    "# 4) title/description/url 기본 정리(공백 정리 + NULL_MARKERS → NaN)\n",
    "for col in [\"title\", \"description\", \"url\"]:\n",
    "    if col in df1.columns:\n",
    "        df1[col] = df1[col].map(normalize_nulls)\n",
    "    else:\n",
    "        # 없으면 만들어 두되 NaN으로 처리\n",
    "        df1[col] = np.nan\n",
    "\n",
    "before1 = len(df1)\n",
    "\n",
    "# 5) description 결측 행 제거\n",
    "df1 = df1.dropna(subset=[\"description\"])\n",
    "\n",
    "# 6) 중복 행 제거 (title+description 기준)\n",
    "# df = df.drop_duplicates(subset=[\"title\", \"description\"])\n",
    "df1 = df1.drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "# (선택) title도 비어있으면 제거\n",
    "# df = df.dropna(subset=[\"title\"])\n",
    "\n",
    "after1 = len(df1)\n",
    "removed1 = before1 - after1\n",
    "\n",
    "# 7) 저장\n",
    "df1.to_csv(DST1, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\" 전처리 완료: {removed1}행 제거 → {after1}행 남김\")\n",
    "print(f\"저장 위치: {os.path.abspath(DST1)}\")\n",
    "# 필요시 미리보기\n",
    "# display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8267a8d7-ef85-412e-ab23-2070ef1458d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>reason</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>코로나 대전된 PVDF 필름의 열자격 전류</td>\n",
       "      <td>본 논문에서는 두께 50[㎛]의 미연신 α형 폴리비닐덴 후로라이드 [Polyviny...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>코로나 대전된 루미롤 필름의 열자격 전류 특성에 관한 연구</td>\n",
       "      <td>온도범위 293~403[˚K]에서 코로나 대전된 루미롤 필름으로부터 열자격전류를 측...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>동물세포에 결합하는 사스 코로나 바이러스 RBD에 관한 연구</td>\n",
       "      <td>사스(Severe Acute Respiratory Syndrome) 코로나 바이러스...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>코로나 바이러스 감염증-19 (COVID-19)가 스포츠 활동에 미치는 영향에 대한...</td>\n",
       "      <td>본 연구는 코로나 바이러스 감염증-19 (COVID-19, 이하 코로나 19)가 스...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fc 결합이 중동호흡기증후군 코로나바이러스 스파이크 단백질의 면역원성에 미치는 영향</td>\n",
       "      <td>중동호흡기증후군 코로나 바이러스는 인간에게 감염되었을 시 사망률이 약 ~36%로 보...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                            코로나 대전된 PVDF 필름의 열자격 전류   \n",
       "1                   코로나 대전된 루미롤 필름의 열자격 전류 특성에 관한 연구   \n",
       "2                  동물세포에 결합하는 사스 코로나 바이러스 RBD에 관한 연구   \n",
       "3  코로나 바이러스 감염증-19 (COVID-19)가 스포츠 활동에 미치는 영향에 대한...   \n",
       "4     Fc 결합이 중동호흡기증후군 코로나바이러스 스파이크 단백질의 면역원성에 미치는 영향   \n",
       "\n",
       "                                         description  \\\n",
       "0  본 논문에서는 두께 50[㎛]의 미연신 α형 폴리비닐덴 후로라이드 [Polyviny...   \n",
       "1  온도범위 293~403[˚K]에서 코로나 대전된 루미롤 필름으로부터 열자격전류를 측...   \n",
       "2  사스(Severe Acute Respiratory Syndrome) 코로나 바이러스...   \n",
       "3  본 연구는 코로나 바이러스 감염증-19 (COVID-19, 이하 코로나 19)가 스...   \n",
       "4  중동호흡기증후군 코로나 바이러스는 인간에게 감염되었을 시 사망률이 약 ~36%로 보...   \n",
       "\n",
       "                                                 url  score  reason  level  \n",
       "0  http://click.ndsl.kr/servlet/OpenAPIDetailView...    NaN     NaN    NaN  \n",
       "1  http://click.ndsl.kr/servlet/OpenAPIDetailView...    NaN     NaN    NaN  \n",
       "2  http://click.ndsl.kr/servlet/OpenAPIDetailView...    NaN     NaN    NaN  \n",
       "3  http://click.ndsl.kr/servlet/OpenAPIDetailView...    NaN     NaN    NaN  \n",
       "4  http://click.ndsl.kr/servlet/OpenAPIDetailView...    NaN     NaN    NaN  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = pd.read_csv(\"papers_clean.prep.csv\",encoding='utf-8-sig')\n",
    "prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dce21411-9f0b-4e73-bb7e-3d501fe92338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>keywords</th>\n",
       "      <th>org</th>\n",
       "      <th>year</th>\n",
       "      <th>url</th>\n",
       "      <th>doi</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>de776da1a1d0f302eeb7608c32c810d3</td>\n",
       "      <td>This item has been removed from publication</td>\n",
       "      <td>En este archivo se encuentran los resultados d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>OpenAIRE</td>\n",
       "      <td>9999</td>\n",
       "      <td>https://dx.doi.org/10.71590/rsfwiq/0yfh7w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18fce9b918b6abc277517b4a065df9e5</td>\n",
       "      <td>Homemade Toys</td>\n",
       "      <td>Supported by funding from the Department of Ar...</td>\n",
       "      <td>['Toys;Bréagáin;An Baile Dubh;Ballyduff']</td>\n",
       "      <td>OpenAIRE</td>\n",
       "      <td>7938</td>\n",
       "      <td>https://dx.doi.org/10.7925/drs1.duchas_4504414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a8331499d9a5e7796fdfa528657b7143</td>\n",
       "      <td>Local Forge</td>\n",
       "      <td>Supported by funding from the Department of Ar...</td>\n",
       "      <td>['Smithing;Gaibhneacht']</td>\n",
       "      <td>OpenAIRE</td>\n",
       "      <td>4938</td>\n",
       "      <td>https://dx.doi.org/10.7925/drs1.duchas_4927495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e81bb07a6b450ac61e926e8b27d0958e</td>\n",
       "      <td>Agta hunter-gatherer oral microbiomes are shap...</td>\n",
       "      <td>Here we investigate the effects of extensive s...</td>\n",
       "      <td>['Hunter-gatherers;Oral microbiome;Disease spr...</td>\n",
       "      <td>OpenAIRE</td>\n",
       "      <td>2030</td>\n",
       "      <td>https://dx.doi.org/10.5281/zenodo.6338839;http...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03a0e4393dd46936b3b5d62d3a902860</td>\n",
       "      <td>Local Fairs</td>\n",
       "      <td>Supported by funding from the Department of Ar...</td>\n",
       "      <td>['Commerce;Buying and selling;Díol agus ceanna...</td>\n",
       "      <td>OpenAIRE</td>\n",
       "      <td>2029</td>\n",
       "      <td>https://dx.doi.org/10.7925/drs1.duchas_5191737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  de776da1a1d0f302eeb7608c32c810d3   \n",
       "1  18fce9b918b6abc277517b4a065df9e5   \n",
       "2  a8331499d9a5e7796fdfa528657b7143   \n",
       "3  e81bb07a6b450ac61e926e8b27d0958e   \n",
       "4  03a0e4393dd46936b3b5d62d3a902860   \n",
       "\n",
       "                                               title  \\\n",
       "0        This item has been removed from publication   \n",
       "1                                      Homemade Toys   \n",
       "2                                        Local Forge   \n",
       "3  Agta hunter-gatherer oral microbiomes are shap...   \n",
       "4                                        Local Fairs   \n",
       "\n",
       "                                         description  \\\n",
       "0  En este archivo se encuentran los resultados d...   \n",
       "1  Supported by funding from the Department of Ar...   \n",
       "2  Supported by funding from the Department of Ar...   \n",
       "3  Here we investigate the effects of extensive s...   \n",
       "4  Supported by funding from the Department of Ar...   \n",
       "\n",
       "                                            keywords       org  year  \\\n",
       "0                                                 []  OpenAIRE  9999   \n",
       "1          ['Toys;Bréagáin;An Baile Dubh;Ballyduff']  OpenAIRE  7938   \n",
       "2                           ['Smithing;Gaibhneacht']  OpenAIRE  4938   \n",
       "3  ['Hunter-gatherers;Oral microbiome;Disease spr...  OpenAIRE  2030   \n",
       "4  ['Commerce;Buying and selling;Díol agus ceanna...  OpenAIRE  2029   \n",
       "\n",
       "                                                 url  doi lang  \n",
       "0          https://dx.doi.org/10.71590/rsfwiq/0yfh7w  NaN   es  \n",
       "1     https://dx.doi.org/10.7925/drs1.duchas_4504414  NaN   en  \n",
       "2     https://dx.doi.org/10.7925/drs1.duchas_4927495  NaN   en  \n",
       "3  https://dx.doi.org/10.5281/zenodo.6338839;http...  NaN   en  \n",
       "4     https://dx.doi.org/10.7925/drs1.duchas_5191737  NaN   en  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets_clean_prep.csv\",encoding='utf-8-sig')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eeed8a-bf41-4ca2-bddc-4a47b67fa253",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SBERT 사용을 위한 numpy 다운그레이드, tensorflow 삭제 및 pytorch 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf30ff5-1e89-4c5b-b7bf-a42a6c90c4fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (25.0)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.0\n",
      "    Uninstalling pip-25.0:\n",
      "      Successfully uninstalled pip-25.0\n",
      "Successfully installed pip-25.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pip.exe, pip3.10.exe and pip3.exe are installed in 'C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "keras-hub 0.22.1 requires keras>=3.8, which is not installed.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\python.exe -m pip install --upgrade pip\n",
      "WARNING: Skipping tensorflow as it is not installed.\n",
      "WARNING: Skipping tensorflow-gpu as it is not installed.\n",
      "WARNING: Skipping tensorboard as it is not installed.\n",
      "WARNING: Skipping keras as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu129\n",
      "Requirement already satisfied: torch in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (2.8.0+cu129)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (0.23.0+cu129)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (2.8.0+cu129)\n",
      "Requirement already satisfied: filelock in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from sentence-transformers) (4.54.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from sentence-transformers) (2.8.0+cu129)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ogod3\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# TF 강제 로딩 방지 (sentence-transformers는 PyTorch만 필요)\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "# (권장) pip 업데이트\n",
    "!{sys.executable} -m pip install -U pip\n",
    "\n",
    "# (권장) NumPy 호환 버전으로 고정\n",
    "!{sys.executable} -m pip install --upgrade --force-reinstall \"numpy==1.26.4\"\n",
    "\n",
    "# (권장) TF 계열 제거 — SBERT에는 불필요하고 종종 충돌 유발\n",
    "!{sys.executable} -m pip uninstall -y tensorflow tensorflow-gpu tensorboard keras || echo \"no tf to remove\"\n",
    "\n",
    "# PyTorch (CUDA 12.9 전용)\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129\n",
    "\n",
    "# Sentence-Transformers\n",
    "!{sys.executable} -m pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "302091d4-0e73-43fb-ad25-6cf6563866e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cu129\n",
      "cuda available: True\n",
      "cuda version: 12.9\n",
      "gpu: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# pytorch 기반 cuda 버전 확인\n",
    "\n",
    "import torch\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda version:\", torch.version.cuda)\n",
    "print(\"gpu:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32b27f-cd5a-4376-8024-d4c41cf3d2ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sentence Transformer & Cross Encoder 모델 설치(필요x,로컬 폴더 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d58cdf3-32de-437b-b532-4e14c4bdb69c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff1c69ee7154d8b8d98e3afa108e742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ogod3\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7661d6d80f44fdbe74465c678fd309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c16a8c6e2646fb9f11cc5cd9723a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5340bae1b0964d6eabb1e83388366139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2ead6621db4bb7af9c58795a06ca3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02d4663335c424ca54faf0f94b1ab9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3923c7fd063545699f187a397fa98997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d4e7abbeae4deca5f703747d7985e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48f0e29897b47d682ad268985eaa5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac8b3d1c6bb4385a8c1f4f4f3cf627c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SBERT model to: local_sbert_models/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "save_dir = \"local_sbert_models/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# 모델 다운로드 + 저장\n",
    "model = SentenceTransformer(model_name)\n",
    "model.save(save_dir)\n",
    "print(\"Saved SBERT model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d15b8593-ab93-492d-9b9c-e38680558605",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SBERT = True\n",
    "SBERT_MODEL_NAME_OR_PATH = \"models/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# SentenceTransformer(SBERT_MODEL_NAME_OR_PATH) 사용하면 Hugging Face 허브가 아니라 로컬 폴더에서 LLM 불러오는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff3093fb-29aa-4813-8b31-37129fd42c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\models\\paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "save_dir = os.path.abspath(\"models/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "m = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "m.save(save_dir)\n",
    "print(\"saved to:\", save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae1af7e3-8a4d-441c-803c-a720f119d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(save_dir, local_files_only=True)  # 오프라인 로드 강제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d688c18d-872b-42fb-a2d0-7b585b3bc946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ogod3\\miniconda3\\envs\\[gpu]\\lib\\site-packages\\huggingface_hub\\file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b4ec56ae6c4d4cbafb97aed541a212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e720d5c43343268f523ff5d25d9511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMTEB-retrieval-bge-zh-v1.5.png:   0%|          | 0.00/51.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd70abddb0544864a630913f8751e644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "miracl-bge-m3.png:   0%|          | 0.00/52.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e838782bc924b7b90f8fa716815c369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BEIR-e5-mistral.png:   0%|          | 0.00/40.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9676a23adeb84816a8067256e1c60016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9b4fae86164241b153bb351a3853c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BEIR-bge-en-v1.5.png:   0%|          | 0.00/56.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a39e9d81944ae5a29ddfb38259f4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama-index.png:   0%|          | 0.00/106k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\models\\bge-reranker-v2-m3\n"
     ]
    }
   ],
   "source": [
    "# ↓ 1회만 실행\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MODEL_ID  = \"BAAI/bge-reranker-v2-m3\"\n",
    "LOCAL_DIR = os.path.join(\"models\", \"bge-reranker-v2-m3\")  # 원하는 경로\n",
    "\n",
    "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
    "\n",
    "# Windows에서 symlink 경고/문제 피하기: local_dir_use_symlinks=False\n",
    "snapshot_download(\n",
    "    repo_id=MODEL_ID,\n",
    "    local_dir=LOCAL_DIR,\n",
    "    local_dir_use_symlinks=False,   # ← 복사본 저장(심볼릭링크 X)\n",
    "    resume_download=True\n",
    ")\n",
    "\n",
    "print(\"Saved to:\", os.path.abspath(LOCAL_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb969b9-7acd-4932-b50a-d5c589070383",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6328c9-dbd9-4519-8051-39fcce197a8b",
   "metadata": {},
   "source": [
    "## 캐시 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e1a322b-0f94-4a33-8c7a-19d9337f1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Global caches ====\n",
    "_BM25_P = None\n",
    "_BM25_D = None\n",
    "_P_DENSE_TEXTS = None\n",
    "_D_DENSE_TEXTS = None\n",
    "_P_DENSE_VECS  = None\n",
    "_D_DENSE_VECS  = None\n",
    "\n",
    "def _ensure_indexes_and_dense(papers_df, datasets_df, backend):\n",
    "    \"\"\"세션 동안 1회만 구축해서 재사용\"\"\"\n",
    "    global _BM25_P, _BM25_D, _P_DENSE_TEXTS, _D_DENSE_TEXTS, _P_DENSE_VECS, _D_DENSE_VECS\n",
    "\n",
    "    if _BM25_P is None:\n",
    "        _BM25_P = WeightedBM25(papers_df, FIELD_WEIGHTS)\n",
    "    if _BM25_D is None:\n",
    "        _BM25_D = WeightedBM25(datasets_df, FIELD_WEIGHTS)\n",
    "\n",
    "    if _P_DENSE_VECS is None:\n",
    "        if \"__dense_text__\" not in papers_df.columns:\n",
    "            papers_df[\"__dense_text__\"] = [compose_dense_text(r) for _, r in papers_df.iterrows()]\n",
    "        _P_DENSE_TEXTS = papers_df[\"__dense_text__\"].tolist()\n",
    "        _P_DENSE_VECS  = backend.encode(_P_DENSE_TEXTS)\n",
    "\n",
    "    if _D_DENSE_VECS is None:\n",
    "        if \"__dense_text__\" not in datasets_df.columns:\n",
    "            datasets_df[\"__dense_text__\"] = [compose_dense_text(r) for _, r in datasets_df.iterrows()]\n",
    "        _D_DENSE_TEXTS = datasets_df[\"__dense_text__\"].tolist()\n",
    "        _D_DENSE_VECS  = backend.encode(_D_DENSE_TEXTS)\n",
    "\n",
    "def reset_retrieval_cache():\n",
    "    \"\"\"코퍼스를 바꾸면 호출해서 캐시 초기화\"\"\"\n",
    "    global _BM25_P, _BM25_D, _P_DENSE_TEXTS, _D_DENSE_TEXTS, _P_DENSE_VECS, _D_DENSE_VECS\n",
    "    _BM25_P = _BM25_D = _P_DENSE_TEXTS = _D_DENSE_TEXTS = _P_DENSE_VECS = _D_DENSE_VECS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf9431-5ce5-40dd-96b5-f967e2ee4ef1",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f87aa91f-5907-46c7-8378-e02cf5631eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 논문/데이터셋 추천 (입력: 제목+설명 직접 타이핑)\n",
    "# - 입력: papers_clean.csv (columns: title, description, url)\n",
    "# - 질의 입력: input()으로 제목/설명 별도 입력\n",
    "# - 임베딩: 기본 TF-IDF(char 2~4그램) / (옵션) 로컬 SBERT\n",
    "# - 질의 벡터: 제목/설명 임베딩을 가중합(W_TITLE, W_DESC)\n",
    "# - 출력: 상위 TOPK개 (rank, title, description, url, score, reason, level)\n",
    "# - 저장: recommendations.csv\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from IPython.display import display\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "PAPERS_CSV    = \"papers_clean.prep.csv\"\n",
    "DATASETS_CSV  = \"datasets_clean_prep.csv\"   # 새로 추가\n",
    "OUTPUT_PAPERS = \"recommendations.papers.csv\"\n",
    "OUTPUT_DATA   = \"recommendations.datasets.csv\"\n",
    "TOPK          = 5\n",
    "\n",
    "# 제목/설명 가중치\n",
    "W_TITLE = 1.0\n",
    "W_DESC  = 2.0\n",
    "\n",
    "USE_SBERT = True\n",
    "SBERT_MODEL_NAME_OR_PATH = \"models/paraphrase-multilingual-MiniLM-L12-v2\"  # 로컬에 받은 경로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be7cc0-a593-4db5-8560-7b5d920b2f50",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c87dad3-1af4-41d2-8fca-6ca81395743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------Explanation----------------------------\n",
    "def safe_text(x) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return str(x)\n",
    "def get_tokenizer(USE_SBERT: bool):\n",
    "    \"\"\"\n",
    "    SBERT 모드에서는 토크나이저 skip.\n",
    "    TF-IDF 모드일 때만 Mecab/Fallback 토크나이저 사용.\n",
    "    \"\"\"\n",
    "    if USE_SBERT:\n",
    "        # SBERT 내부 토크나이저 사용 → 외부 tokenizer 필요 없음\n",
    "        def dummy_tok(text: str):\n",
    "            return []  # build_reason 등에선 공백 리스트 반환\n",
    "        return dummy_tok, \"skip\"\n",
    "\n",
    "# 간단 한국어/영문 공통 토크나이저 + 불용어\n",
    "_STOP = {\"및\",\"과\",\"와\",\"에서\",\"으로\",\"으로의\",\"대한\",\"관련\",\"하는\",\"하기\",\"이다\",\"있는\",\"위한\",\n",
    "         \"the\",\"a\",\"an\",\"and\",\"of\",\"to\",\"in\",\"for\",\"on\",\"with\",\"by\",\"about\",\"at\",\"as\",\"is\",\"are\"}\n",
    "\n",
    "_rx = re.compile(r\"[가-힣A-Za-z0-9]+\")\n",
    "\n",
    "def _lite_tokens(s: str):\n",
    "    toks = [t.lower() for t in _rx.findall(s or \"\")]\n",
    "    return [t for t in toks if t not in _STOP and len(t) > 1]\n",
    "\n",
    "def label_by_ranked_score(scores: np.ndarray, i: int) -> str:\n",
    "    # 상위 5개 안에서 상대 기준 (예: 최상위 0~1 구간 정규화)\n",
    "    smax, smin = float(scores[0]), float(scores[min(len(scores)-1, 4)])\n",
    "    denom = max(1e-6, smax - smin)\n",
    "    rel = (float(scores[i]) - smin) / denom  # 0~1\n",
    "    if rel >= 0.75: return \"강추\"\n",
    "    if rel >= 0.45: return \"추천\"\n",
    "    return \"참고\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9a61f-b0f2-4c16-a41f-9f56b5792a56",
   "metadata": {},
   "source": [
    "## Embedding backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac86000b-c309-4918-8a2f-e757f7e4c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Embedding backends --------------------\n",
    "class EmbeddingBackend:\n",
    "    def fit(self, texts: List[str]): ...\n",
    "    def encode(self, texts: List[str]): ...\n",
    "\n",
    "class SBERTBackend:\n",
    "    def __init__(self, model_path: str):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.model = SentenceTransformer(model_path)\n",
    "\n",
    "    def fit(self, texts):  # SBERT는 학습 필요 없음\n",
    "        pass\n",
    "\n",
    "    def encode(self, texts):\n",
    "        vecs = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=32,\n",
    "            show_progress_bar=False,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        return vecs\n",
    "\n",
    "def get_backend():\n",
    "    if USE_SBERT:\n",
    "        return SBERTBackend(SBERT_MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13532c14-74d5-45f0-89ff-d6ae61b47420",
   "metadata": {},
   "source": [
    "## 추출적 근거(Extractive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "770c915f-c8bc-4b58-8c65-6de80a8d6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 초경량 추출형 추천사유: 입력과 가장 유사한 '한 문장' ====\n",
    "import re, numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "MAX_REASON_CHARS = 100\n",
    "_rx_split = re.compile(r\"(?<=[.!?。？！])\\s+|[\\r\\n]+|[•\\u2022]\")\n",
    "\n",
    "def extractive_reason(q_title: str, q_desc: str,\n",
    "                      doc_title: str, doc_desc: str,\n",
    "                      backend, max_chars: int = MAX_REASON_CHARS) -> str:\n",
    "    \"\"\"입력(제목+설명)과 후보 설명을 비교해 유사도가 가장 큰 문장 1개를 반환\"\"\"\n",
    "    raw = (doc_desc or \"\").strip() or (doc_title or \"\")\n",
    "    cands = [s.strip() for s in _rx_split.split(raw) if s and len(s.strip()) > 2]\n",
    "    cands = cands[:5]  # 너무 많은 문장 비교 방지 (속도)\n",
    "    if not cands:\n",
    "        s = (doc_title or \"\").strip()\n",
    "        return tidy_korean_sentence(s, max_chars)\n",
    "\n",
    "    # 쿼리/문장 임베딩\n",
    "    q_text = f\"{(q_title or '').strip()} {(q_desc or '').strip()}\".strip()\n",
    "    qv = backend.encode([q_text])\n",
    "    sv = backend.encode(cands)\n",
    "\n",
    "    # 코사인 유사도 (정규화 임베딩 가정 → 내적 사용, 실패 시 fallback)\n",
    "    try:\n",
    "        from scipy import sparse as _sp\n",
    "        if hasattr(qv, \"shape\") and len(getattr(qv, \"shape\", [])) == 1:\n",
    "            qv = qv.reshape(1, -1)\n",
    "        sims = (qv @ (sv.T if not _sp.issparse(sv) else sv.T)).toarray().ravel() \\\n",
    "               if _sp.issparse(sv) else (qv @ sv.T).ravel()\n",
    "    except Exception:\n",
    "        sims = cosine_similarity(qv, sv)[0]\n",
    "\n",
    "    # 최댓값부터 후보 인덱스 정렬\n",
    "    order = list(np.argsort(-sims))\n",
    "    best = None\n",
    "    \n",
    "    for idx in order[:3]:  # 최상위 3개 중에서 너무 겹치지 않는 문장 선택\n",
    "        cand = cands[int(idx)]\n",
    "        para = _light_paraphrase_ko(cand)\n",
    "        # 원문과 너무 비슷하면(겹침률 0.95↑) 다음 후보 시도\n",
    "        if _overlap_ratio(para, cand) >= 0.95:\n",
    "            continue\n",
    "        best = para\n",
    "        break\n",
    "    \n",
    "    if best is None:\n",
    "        # 전부 비슷하면 최상위 문장만 가볍게 손질\n",
    "        best = _light_paraphrase_ko(cands[int(order[0])])\n",
    "    \n",
    "    best = re.sub(r\"\\s+\", \" \", best).strip()\n",
    "    return tidy_korean_sentence(best, max_chars)\n",
    "\n",
    "# ==== 한국어 문장 정리(맞춤법/문장부호 최소 정돈) ====\n",
    "import re, html, unicodedata\n",
    "\n",
    "_rx_multi_space = re.compile(r\"\\s+\")\n",
    "def tidy_korean_sentence(text: str, max_chars: int = 100) -> str:\n",
    "    t = html.unescape(text or \"\")                 # &#x00B7; 등 HTML 엔티티 해제\n",
    "    t = unicodedata.normalize(\"NFKC\", t)          # 전각/호환 문자 정규화\n",
    "\n",
    "    # 불필요한 제로폭/제어문자 제거\n",
    "    t = re.sub(r\"[\\u200B-\\u200D\\uFEFF]\", \"\", t)\n",
    "\n",
    "    # 괄호 안/앞뒤 공백 정리\n",
    "    t = re.sub(r\"\\(\\s+\", \"(\", t)\n",
    "    t = re.sub(r\"\\s+\\)\", \")\", t)\n",
    "\n",
    "    # 구두점 앞 공백 제거, 뒤는 한 칸\n",
    "    t = re.sub(r\"\\s+([,\\.!?;:)\\]])\", r\"\\1\", t)\n",
    "    t = re.sub(r\"([,;:])(?=\\S)\", r\"\\1 \", t)\n",
    "\n",
    "    # , . 순서/중복 구두점 정리\n",
    "    t = re.sub(r\",\\s*\\.\", \".\", t)\n",
    "    t = re.sub(r\"\\.\\s*,\", \".\", t)\n",
    "    t = re.sub(r\"([\\.!?,])\\1+\", r\"\\1\", t)\n",
    "\n",
    "    # 리스트 점/기호류 가볍게 교정\n",
    "    t = t.replace(\"•\", \"·\").replace(\"・\", \"·\")\n",
    "    t = re.sub(r\"\\s*·\\s*\", \"·\", t)\n",
    "\n",
    "    # 다중 공백 정리\n",
    "    t = _rx_multi_space.sub(\" \", t).strip()\n",
    "\n",
    "    # 길이 제한 및 종결 보정\n",
    "    t = t[:max_chars].rstrip()\n",
    "    if not t.endswith((\"다\", \"요\", \"임\", \"함\", \".\", \"!\", \"?\")):\n",
    "        t += \".\"\n",
    "\n",
    "    return t\n",
    "\n",
    "# (), [], {}, <>, 〈〉, 《》, 「」, 『』, 【】, 〔〕 등 1층 괄호 블록 제거\n",
    "_rx_paren_any = re.compile(r\"\\s*[\\(\\[\\{<〈《「『【〔]\\s*[^)\\]\\}>〉》」』】〕]{0,200}\\s*[\\)\\]\\}>〉》」』】〕]\\s*\")\n",
    "\n",
    "def drop_paren_glue(s: str) -> str:\n",
    "    \"\"\"괄호 안 문구를 모두 제거하고 공백/구두점 정리\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    t = str(s)\n",
    "\n",
    "    # 중첩 괄호 대비: 더 이상 치환이 안 될 때까지 반복\n",
    "    prev = None\n",
    "    while prev != t:\n",
    "        prev = t\n",
    "        t = _rx_paren_any.sub(\" \", t)\n",
    "\n",
    "    # 공백 뭉침/구두점 주변 공백 정리\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+([,\\.!?;:])\", r\"\\1\", t)   # 구두점 앞 공백 제거\n",
    "    t = re.sub(r\"([,;:])(?=\\S)\", r\"\\1 \", t)   # 구두점 뒤 한 칸\n",
    "    return t.strip()\n",
    "\n",
    "# 흔한 서두/표현 치환(아주 보수적)\n",
    "_REP = [\n",
    "    (r\"^(본\\s*연구|이\\s*연구|본\\s*논문|이\\s*논문|본\\s*문서|이\\s*문서)\\s*(는|에서는)\\s*\", \"\"),  # 서두 삭제\n",
    "    (r\"다룬다\", \"분석한다\"),\n",
    "    (r\"보여준다\", \"확인했다\"),\n",
    "    (r\"제시한다\", \"제안한다\"),\n",
    "    (r\"탐구한다\", \"살핀다\"),\n",
    "    (r\"효과를\\s*보였다\", \"효과를 확인했다\"),\n",
    "    (r\"\\s*·\\s*\", \"·\"),\n",
    "]\n",
    "def _light_paraphrase_ko(s: str) -> str:\n",
    "    t = drop_paren_glue(s)\n",
    "    for p, r in _REP:\n",
    "        t = re.sub(p, r, t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def _overlap_ratio(a: str, b: str) -> float:\n",
    "    # 토큰 겹침 비율(간단 자카드) – 너무 같으면 다른 문장/치환 시도\n",
    "    import re\n",
    "    tok = lambda x: set(w for w in re.findall(r\"[가-힣A-Za-z0-9]+\", x.lower()) if len(w) > 1)\n",
    "    ta, tb = tok(a), tok(b)\n",
    "    return (len(ta & tb) / max(1, len(ta))) if ta else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aac9d1-bcf0-4b40-a5e2-8e55a07c6340",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LLM prompting(폐기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "444d25cf-8aaf-4dd7-8afe-9706babfbdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== LLM 기반 '추천 사유' 생성 셀 (Option A: causal LLM) ====\n",
    "# import torch, re\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# USE_LLM_REASON = True\n",
    "# LLM_ID = \"models/Qwen2.5-1.5B-Instruct\"   # 로컬 폴더 or HF repo id\n",
    "# MAX_REASON_CHARS = 60                    # 총 글자수 한도(두 문장 이내)\n",
    "\n",
    "# _llm_pipe = None\n",
    "# def _load_llm_causal():\n",
    "#     global _llm_pipe\n",
    "#     if _llm_pipe is not None:\n",
    "#         return _llm_pipe\n",
    "\n",
    "#     tok = AutoTokenizer.from_pretrained(\n",
    "#         LLM_ID, use_fast=True, trust_remote_code=True\n",
    "#     )\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         LLM_ID,\n",
    "#         trust_remote_code=True,\n",
    "#         torch_dtype=(torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "#         # device_map=\"auto\"  # ← 가속 패키지 없이도 되도록 제거\n",
    "#     )\n",
    "#     if torch.cuda.is_available():\n",
    "#         model = model.to(\"cuda\")\n",
    "\n",
    "#     if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
    "#         tok.pad_token_id = tok.eos_token_id\n",
    "\n",
    "#     _llm_pipe = pipeline(\n",
    "#         \"text-generation\",\n",
    "#         model=model,\n",
    "#         tokenizer=tok,\n",
    "#         device=(0 if torch.cuda.is_available() else -1),\n",
    "#     )\n",
    "#     return _llm_pipe\n",
    "\n",
    "# # --- few-shot 예시(두 문장·100자 이내로 축약) ---\n",
    "# FEWSHOTS = [\n",
    "#     {\n",
    "#         \"query_t\": \"물 부족 도시를 위한 물 순환경제 : 물 시스템의 물 대사, 에너지, 탄소 분석\",\n",
    "#         \"query_d\": \"파주시 물 대사를 통해 재이용·에너지·탄소 상호작용을 정량 분석하고 전력 탈탄소화 필요를 제시.\",\n",
    "#         \"doc_t\": \"Toward Carbon-Neutral Water Systems: Insights from Global Cities\",\n",
    "#         \"doc_sents\": [\n",
    "#             \"여러 도시의 상하수도 부문에서 온실가스 감축 사례와 부문 간 협력 전략을 제시한다.\"\n",
    "#         ],\n",
    "#         \"why\": \"도시 물 시스템의 탄소감축 전략을 실증해 재이용·전력 탈탄소화 필요성과 직접 맞닿습니다.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"query_t\": \"코로나19 이후 소비트렌드 변화에 따른 지식재산권 관련 이슈 및 시사점\",\n",
    "#         \"query_d\": \"팬데믹으로 온라인 소비가 급증하고 IP 리스크·위조품 유통이 확대된 변화를 다룸.\",\n",
    "#         \"doc_t\": \"Illicit Trade in Fakes under the COVID-19\",\n",
    "#         \"doc_sents\": [\n",
    "#             \"전자상거래 급증이 위조품 확산과 단속 과제에 미친 영향을 분석한다.\"\n",
    "#         ],\n",
    "#         \"why\": \"온라인 전환에서 IP 침해 확대라는 인과가 동일해 정책·집행 시사점이 직접 연결됩니다.\"\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# def _fewshots_block():\n",
    "#     lines = []\n",
    "#     for ex in FEWSHOTS:\n",
    "#         lines += [\n",
    "#             \"[사용자]\",\n",
    "#             f'입력(제목+설명): \"{ex[\"query_t\"]} || {ex[\"query_d\"]}\"',\n",
    "#             f'후보 문서: 제목=\"{ex[\"doc_t\"]}\"',\n",
    "#             \"관련성이 높은 문장:\",\n",
    "#             *[f\"- {s}\" for s in ex[\"doc_sents\"]],\n",
    "#             \"[에이전트]\",\n",
    "#             ex[\"why\"],\n",
    "#         ]\n",
    "#     return \"\\n\".join(lines)\n",
    "\n",
    "# # 금지 기호 제거 + 두 문장 제한 + 길이 제한 + 마침표 보정 + ‘베끼기’ 완화\n",
    "# _FORBIDDEN = r\"[=\\-•·\\*\\_\\+\\|\\[\\]\\{\\}\\(\\)~^<>/\\\\:;]+\"  # 금지 기호 정규식\n",
    "# _SENT_SPLIT = r\"(?<=[\\.!?])\\s+\"\n",
    "\n",
    "# def _truncate_kr(s: str, limit=MAX_REASON_CHARS) -> str:\n",
    "#     s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "#     return s[:limit]\n",
    "\n",
    "# # 금지 기호/문장 분리 정규식은 기존 변수(_FORBIDDEN, _SENT_SPLIT) 그대로 사용\n",
    "# def _sanitize_reason(\n",
    "#     text: str,\n",
    "#     limit: int = MAX_REASON_CHARS,\n",
    "#     doc_raw: str | None = None,\n",
    "#     ban_phrases: tuple[str, ...] = (),\n",
    "# ) -> str:\n",
    "#     import re\n",
    "#     # 기호 제거 + 공백 정리\n",
    "#     text = re.sub(_FORBIDDEN, \" \", text)\n",
    "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "#     # 문장 후보들\n",
    "#     sents = [s.strip() for s in re.split(_SENT_SPLIT, text) if s.strip()]\n",
    "#     # 제목 되풀이/상투어(“…글입니다”)가 아닌 첫 문장 선택\n",
    "#     def ok(s: str) -> bool:\n",
    "#         low = s.lower()\n",
    "#         if any(bp and bp.lower() in low for bp in ban_phrases): return False\n",
    "#         if low.endswith((\"글입니다\", \"논문입니다\", \"데이터입니다\")): return False\n",
    "#         return True\n",
    "\n",
    "#     first = next((s for s in sents if ok(s)), (sents[0] if sents else \"\"))\n",
    "#     text = (first or \"\").strip()[:limit]\n",
    "\n",
    "#     # 문서 설명을 거의 베껴 쓰면 템플릿으로 교체\n",
    "#     if doc_raw:\n",
    "#         def toks(t): \n",
    "#             return set(re.findall(r\"[가-힣A-Za-z0-9]+\", (t or \"\").lower()))\n",
    "#         qt, dt = toks(text), toks(doc_raw)\n",
    "#         if qt and len(qt & dt) / max(1, len(qt)) > 0.85:\n",
    "#             text = \"입력의 문제의식과 후보의 분석 초점이 겹쳐 실제 활용 맥락에서 높은 연관성이 있습니다.\"\n",
    "#             text = text[:limit]\n",
    "\n",
    "#     # 종결 보정\n",
    "#     if text and not text.endswith((\"다\",\"요\",\"임\",\"함\",\".\",\"!\",\"?\")):\n",
    "#         text += \".\"\n",
    "#     return text\n",
    "\n",
    "# def llm_reason(query_title, query_desc, doc_title, doc_desc, picked_sents=None):\n",
    "#     \"\"\"\n",
    "#     picked_sents: 문서에서 골라낸 1문장(없으면 doc_desc로 임시 추출)\n",
    "#     \"\"\"\n",
    "#     if not USE_LLM_REASON:\n",
    "#         return None\n",
    "\n",
    "#     pipe = _load_llm_causal()\n",
    "#     sys = \"너는 국내외 연구데이터에 대한 연관 논문,데이터 추천 에이전트다.\"\n",
    "\n",
    "#     # 문서에서 근거 문장 1~2개 추출(없으면 설명에서 임시 분할)\n",
    "#     picked_sents = (picked_sents or [])[:2]\n",
    "#     if not picked_sents:\n",
    "#         tmp = re.split(r\"[.!?。？！]\\s+\", (doc_desc or \"\").strip())\n",
    "#         picked_sents = [t for t in tmp if t][:2]\n",
    "\n",
    "#     # 지시사항 업데이트: 기호 금지, 베끼지 말기, 두 문장·100자 이내\n",
    "#     prompt = (\n",
    "#         f\"[시스템] {sys}\\n\"\n",
    "#         + _fewshots_block() + \"\\n\"\n",
    "#         + \"[사용자]\\n\"\n",
    "#         + f'입력(제목+설명): \"{query_title} || {query_desc}\"\\n'\n",
    "#         + f'후보 문서: 제목=\"{doc_title}\"\\n'\n",
    "#         + \"관련성이 높은 문장:\\n\"\n",
    "#         + \"\\n\".join(f\"- {s}\" for s in picked_sents)\n",
    "#         + \"\\n\\n요구사항:\\n\"\n",
    "#         + \"1) 입력 데이터와 후보 데이터가 어떤 내용이 비슷한지 설명하고 설명 열에 있는 내용을 그대로 옮기지 말 것.\\n\"\n",
    "#         + \"2) 키워드 나열 금지, 특정 부분에서 의미가 비슷하다는 내용으로 서술.\\n\"\n",
    "#         + \"3) 기호(=, -, :, •, *, _, / 등) 사용 금지.\\n\"\n",
    "#         + \"4) 한국어로 한 문장 이내, 총 60자 이내로 간결히 작성. 마지막은 마침표로 끝낼 것.\\n\"\n",
    "#         + \"5) 입력/후보 ‘제목’을 반복하거나 글 전체 요약을 하지 말고, 입력의 A와 후보의 B가 어떻게 ‘의미적으로 대응’하는지 한 가지 포인트만 말할 것.\\n\"\n",
    "#         + \"[에이전트]\\n\"\n",
    "#     )\n",
    "\n",
    "#     out = pipe(\n",
    "#         prompt,\n",
    "#         max_new_tokens=80,\n",
    "#         do_sample=False,\n",
    "#         temperature=0.0,\n",
    "#         top_p=1.0,\n",
    "#         repetition_penalty=1.05,\n",
    "#         eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "#     )[0][\"generated_text\"]\n",
    "\n",
    "#     ans_raw = out.split(\"[에이전트]\")[-1].strip()\n",
    "#     doc_raw  = f\"{doc_title} {doc_desc}\"\n",
    "#     ans = _sanitize_reason(\n",
    "#         ans_raw,\n",
    "#         MAX_REASON_CHARS,\n",
    "#         doc_raw=doc_raw,\n",
    "#         ban_phrases=(query_title, doc_title, \"입니다\", \"글입니다\"),\n",
    "#     )\n",
    "\n",
    "#     # 너무 짧게 남으면 안전 템플릿\n",
    "#     if len(ans) < 8:\n",
    "#         qt = (_key_terms(f\"{query_title} {query_desc}\", topn=1) or [\"핵심 주제\"])[0]\n",
    "#         dt = (_key_terms(doc_desc, topn=1) or [\"동일 주제\"])[0]\n",
    "#         ans = f\"입력의 {qt}와 후보의 {dt}가 대응해 맥락상 높은 관련성을 보입니다.\"\n",
    "\n",
    "\n",
    "#     if not ans:\n",
    "#         # 완전 실패 시 백업\n",
    "#         ans = \"연구 대상과 접근 방식이 유사해 실제 활용 맥락에서 높은 연관성이 있습니다.\"\n",
    "#     return ans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a31a46-e6c5-4983-9de2-7faf16be86cf",
   "metadata": {},
   "source": [
    "## 기존 모델(Recommender&cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af4ec050-7088-43c6-98fd-2ab36933e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Recommender --------------------\n",
    "class Recommender:\n",
    "    def __init__(self, df: pd.DataFrame, backend: 'EmbeddingBackend|None' = None):  # 변경\n",
    "        self.df = df.copy()\n",
    "        self.df = self.df.rename(columns={c: c.lower() for c in self.df.columns})\n",
    "        for col in [\"title\", \"description\", \"url\"]:\n",
    "            if col not in self.df.columns:\n",
    "                self.df[col] = \"\"\n",
    "\n",
    "        self.df[\"__text__\"] = (\n",
    "            self.df[\"title\"].map(safe_text) + \" \" + self.df[\"description\"].map(safe_text)\n",
    "        ).fillna(\"\").str.strip()\n",
    "        self.df = self.df[self.df[\"__text__\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "        self.backend = backend or get_backend()   # 외부에서 넘기면 재사용\n",
    "        self.doc_vectors = None\n",
    "\n",
    "    def fit(self):\n",
    "        texts = self.df[\"__text__\"].tolist()\n",
    "        self.backend.fit(texts)\n",
    "        self.doc_vectors = self.backend.encode(texts)\n",
    "\n",
    "    def _cosine(self, A, B) -> np.ndarray:\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        return cosine_similarity(A, B)[0]\n",
    "\n",
    "    def encode_query_weighted(self, title: str, desc: str, w_title: float, w_desc: float):\n",
    "        \"\"\"제목/설명 임베딩을 같은 백엔드로 구해 가중합 → 질의 벡터\"\"\"\n",
    "        t = safe_text(title)\n",
    "        d = safe_text(desc)\n",
    "\n",
    "        vt = self.backend.encode([t])\n",
    "        vd = self.backend.encode([d])\n",
    "\n",
    "        try:\n",
    "            # scipy.sparse 지원(TF-IDF)\n",
    "            from scipy import sparse\n",
    "            if sparse.issparse(vt):\n",
    "                q = vt.multiply(w_title) + vd.multiply(w_desc)\n",
    "            else:\n",
    "                q = vt * w_title + vd * w_desc\n",
    "        except Exception:\n",
    "            q = vt * w_title + vd * w_desc\n",
    "        return q\n",
    "\n",
    "    def recommend(self, title: str, desc: str, topk: int = 5) -> pd.DataFrame:\n",
    "        q_vec = self.encode_query_weighted(title, desc, W_TITLE, W_DESC)\n",
    "        sims = self._cosine(q_vec, self.doc_vectors)\n",
    "        top_idx = np.argsort(-sims)[:topk]\n",
    "\n",
    "        rows = []\n",
    "        top_sims = sims[top_idx]\n",
    "        for i in top_idx:\n",
    "            row = self.df.iloc[i]\n",
    "            score = float(sims[i])\n",
    "\n",
    "            # 레벨 라벨링 유지\n",
    "            level = label_by_ranked_score(top_sims, np.where(top_idx == i)[0][0])\n",
    "\n",
    "            # 추천 사유: 추출형 한 문장으로 교체\n",
    "            if USE_EXTRACTIVE_REASON:\n",
    "                reason = extractive_reason(\n",
    "                    title, desc,\n",
    "                    safe_text(row.get(\"title\",\"\")),\n",
    "                    safe_text(row.get(\"description\",\"\")),\n",
    "                    self.backend,\n",
    "                    max_chars=100,\n",
    "                )\n",
    "            else:\n",
    "                # (옵션) 예전 방식 유지하려면 여기만 남겨두세요.\n",
    "                if TOK_KIND == \"skip\":\n",
    "                    reason_query = f\"{safe_text(title)} {safe_text(desc)}\"\n",
    "                    reason_doc   = f\"{safe_text(row.get('title',''))} {safe_text(row.get('description',''))}\"\n",
    "                    mode = \"skip\"\n",
    "                else:\n",
    "                    reason_query = tokenize(f\"{safe_text(title)} {safe_text(desc)}\")\n",
    "                    reason_doc   = tokenize(row[\"__text__\"])\n",
    "                    mode = TOK_KIND\n",
    "                reason = build_reason(reason_query, reason_doc, mode=mode)\n",
    "\n",
    "            rows.append({\n",
    "                \"rank\": len(rows) + 1,\n",
    "                \"title\": row.get(\"title\", \"\"),\n",
    "                \"description\": row.get(\"description\", \"\"),\n",
    "                \"url\": row.get(\"url\", \"\"),\n",
    "                \"score\": round(score, 4),\n",
    "                \"reason\": reason,\n",
    "                \"level\": level,\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "def load_df(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # 아주 간단한 안전장치\n",
    "    df = df.rename(columns={c: c.lower() for c in df.columns})\n",
    "    for col in [\"title\", \"description\", \"url\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    return df\n",
    "\n",
    "def run_recommendation_for_corpus(df: pd.DataFrame, backend, title: str, desc: str, topk: int) -> pd.DataFrame:\n",
    "    rec = Recommender(df, backend=backend)   # 같은 백엔드 재사용\n",
    "    rec.fit()\n",
    "    return rec.recommend(title, desc, topk=topk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345a97c-80dd-488c-90b2-6fd3f5c4dfa2",
   "metadata": {},
   "source": [
    "## Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "941070f1-5995-4b7c-bfd5-78715cf72dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Multistage Re-ranking (BM25 → Dense → Cross-Encoder) ===\n",
    "import numpy as np, pandas as pd, re, os\n",
    "from typing import Dict, List, Tuple\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---------- Config ----------\n",
    "# 단계별 후보 수\n",
    "TOPN_BM25 = 200   # BM25 1차 후보\n",
    "M_DENSE   = 60    # Dense 재스코어 후 유지\n",
    "L_CE      = 15    # Cross-Encoder 재랭킹 대상\n",
    "K_FINAL   = 5     # 최종 Top-K (3~5 권장)\n",
    "\n",
    "# 점수 결합 가중치 (초기값 제안)\n",
    "ALPHA = 0.35   # BM25 비중\n",
    "BETA  = 0.65   # Dense 비중\n",
    "GAMMA = 0.55   # (BM25+Dense) vs CE 비중\n",
    "\n",
    "# BM25 필드 가중치 (필드 없으면 자동 무시)\n",
    "FIELD_WEIGHTS = {\"title\": 2.0, \"keywords\": 1.6, \"description\": 1.0, \"org\": 0.6, \"doi\": 0.2}\n",
    "\n",
    "# Cross-Encoder 사용 여부 및 모델 (다국어 추천)\n",
    "USE_CE = True\n",
    "CE_MODEL = \"models/bge-reranker-v2-m3\"  \n",
    "\n",
    "# 다국어 이중 쿼리 가중(ko 우선)\n",
    "W_LANG = 0.6   # q* = normalize(W_LANG*q_ko + (1-W_LANG)*q_en)\n",
    "\n",
    "# ---------- Lightweight tokenizer ----------\n",
    "_rx = re.compile(r\"[가-힣A-Za-z0-9]+\")\n",
    "def lite_tokens(s: str) -> List[str]:\n",
    "    return [t.lower() for t in _rx.findall(s or \"\") if len(t) > 1]\n",
    "\n",
    "# ---------- Robust normalization ----------\n",
    "def robust_minmax(x: np.ndarray, lo=5, hi=95) -> np.ndarray:\n",
    "    if len(x) == 0:\n",
    "        return x\n",
    "    a, b = np.percentile(x, lo), np.percentile(x, hi)\n",
    "    denom = max(1e-6, b - a)\n",
    "    return np.clip((x - a) / denom, 0, 1)\n",
    "\n",
    "# ---------- BM25 index per field ----------\n",
    "class WeightedBM25:\n",
    "    \"\"\"각 필드별 BM25를 만들고 가중합으로 점수를 계산\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, fields: Dict[str, float]):\n",
    "        self.fields = {}\n",
    "        for f, w in fields.items():\n",
    "            if f in df.columns:\n",
    "                docs = [lite_tokens(safe_text(x)) for x in df[f].fillna(\"\").astype(str).tolist()]\n",
    "                if sum(len(d) for d in docs) > 0:\n",
    "                    self.fields[f] = (BM25Okapi(docs), w)\n",
    "        self.n_docs = len(df)\n",
    "\n",
    "    def score(self, query_tokens: List[str]) -> np.ndarray:\n",
    "        scores = np.zeros(self.n_docs, dtype=float)\n",
    "        for f, (bm25, w) in self.fields.items():\n",
    "            scores += w * bm25.get_scores(query_tokens)\n",
    "        return scores\n",
    "\n",
    "# ---------- Dense(임베딩) 준비 ----------\n",
    "def compose_dense_text(row: pd.Series) -> str:\n",
    "    # title [SEP] top-8 keywords [SEP] short_desc(최대 300자)\n",
    "    title = safe_text(row.get(\"title\", \"\"))\n",
    "    kws   = safe_text(row.get(\"keywords\", \"\"))\n",
    "    if isinstance(kws, list): kws = \", \".join(kws)\n",
    "    kws = \", \".join(kws.split(\",\")[:8])\n",
    "    desc  = safe_text(row.get(\"description\", \"\"))[:300]\n",
    "    return f\"{title} [SEP] {kws} [SEP] {desc}\".strip()\n",
    "\n",
    "def build_dense_matrix(df: pd.DataFrame, backend) -> Tuple[List[str], np.ndarray]:\n",
    "    texts = [compose_dense_text(r) for _, r in df.iterrows()]\n",
    "    vecs  = backend.encode(texts)  # SBERTBackend.normalize_embeddings=True 권장\n",
    "    return texts, vecs\n",
    "\n",
    "def combine_query_vec(backend, ko_query: str, en_query: str | None) -> np.ndarray:\n",
    "    q_ko = backend.encode([ko_query])[0]\n",
    "    if en_query:\n",
    "        q_en = backend.encode([en_query])[0]\n",
    "        q = W_LANG * q_ko + (1 - W_LANG) * q_en\n",
    "        q = q / (np.linalg.norm(q) + 1e-12)\n",
    "        return q\n",
    "    return q_ko\n",
    "\n",
    "_ce_model_cache = None\n",
    "def ce_predict_pairs(pairs: List[Tuple[str, str]]) -> np.ndarray:\n",
    "    global _ce_model_cache\n",
    "    if not USE_CE:\n",
    "        return np.zeros(len(pairs), dtype=float)\n",
    "\n",
    "    if _ce_model_cache is None:\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        import torch\n",
    "        dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        def _load(model_kwargs=None):\n",
    "            return CrossEncoder(\n",
    "                CE_MODEL, device=dev, max_length=512,\n",
    "                **({\"model_kwargs\": model_kwargs} if model_kwargs else {})\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # 1) SDPA 시도 (가능하면 속도 이점)\n",
    "            _ce_model_cache = _load({\"attn_implementation\": \"sdpa\"})\n",
    "        except Exception:\n",
    "            # 2) SDPA 미지원 시 eager로 폴백\n",
    "            _ce_model_cache = _load({\"attn_implementation\": \"eager\"})\n",
    "\n",
    "    return _ce_model_cache.predict(pairs)\n",
    "\n",
    "# ---------- 파이프라인 본체 ---------- (파이프라인 전체 흐름, 점수 결합 로직)\n",
    "def multistage_recommend(\n",
    "    title_ko: str, desc_ko: str,\n",
    "    papers_df: pd.DataFrame, datasets_df: pd.DataFrame,\n",
    "    backend,\n",
    "    en_title: str | None = None, en_desc: str | None = None,\n",
    "    topk: int = K_FINAL\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # ★ 캐시 보장\n",
    "    _ensure_indexes_and_dense(papers_df, datasets_df, backend)\n",
    "\n",
    "    # 0) 쿼리 문자열\n",
    "    q_ko = (safe_text(title_ko) + \" \" + safe_text(desc_ko)).strip()\n",
    "    q_en = (safe_text(en_title) + \" \" + safe_text(en_desc)).strip() if (en_title or en_desc) else None\n",
    "\n",
    "    # 1) BM25 (캐시 사용)\n",
    "    bm25_p, bm25_d = _BM25_P, _BM25_D\n",
    "    q_tokens = lite_tokens(q_ko) + (lite_tokens(q_en) if q_en else [])\n",
    "    b_p = bm25_p.score(q_tokens); idx_p = np.argsort(-b_p)[:min(TOPN_BM25, len(papers_df))]\n",
    "    b_d = bm25_d.score(q_tokens); idx_d = np.argsort(-b_d)[:min(TOPN_BM25, len(datasets_df))]\n",
    "\n",
    "    # 2) Dense (캐시된 임베딩에서 후보만 참조)\n",
    "    q_vec = combine_query_vec(backend, q_ko, q_en)\n",
    "    s_p_dense = _P_DENSE_VECS[idx_p] @ q_vec\n",
    "    s_d_dense = _D_DENSE_VECS[idx_d] @ q_vec\n",
    "\n",
    "    cand_p = pd.DataFrame({\"src\":\"paper\",\"idx\":idx_p, \"bm25\":b_p[idx_p], \"dense\":s_p_dense})\n",
    "    cand_d = pd.DataFrame({\"src\":\"dataset\",\"idx\":idx_d, \"bm25\":b_d[idx_d], \"dense\":s_d_dense})\n",
    "    cand   = pd.concat([cand_p, cand_d], ignore_index=True).sort_values(\"dense\", ascending=False)\n",
    "    cand   = cand.head(min(M_DENSE, len(cand))).reset_index(drop=True)\n",
    "\n",
    "    # 2.5) 정규화/기본점수\n",
    "    cand[\"bm25_n\"] = robust_minmax(cand[\"bm25\"].to_numpy())\n",
    "    cand[\"dense_n\"] = robust_minmax(cand[\"dense\"].to_numpy())\n",
    "    cand[\"s_base\"]  = ALPHA * cand[\"bm25_n\"] + BETA * cand[\"dense_n\"]\n",
    "\n",
    "    # 3) CE 재랭킹\n",
    "    cand_L = cand.head(min(L_CE, len(cand))).copy()\n",
    "    q_text = q_en if q_en else q_ko\n",
    "    pairs = [\n",
    "        (q_text, (_P_DENSE_TEXTS if src == \"paper\" else _D_DENSE_TEXTS)[int(i)])\n",
    "        for src, i in zip(cand_L[\"src\"], cand_L[\"idx\"])\n",
    "    ]\n",
    "    ce_scores = ce_predict_pairs(pairs) if len(pairs) else np.array([])\n",
    "    cand_L[\"ce\"] = ce_scores if len(ce_scores) else 0.0\n",
    "\n",
    "    # 4) 점수 결합\n",
    "    cand[\"final\"] = cand[\"s_base\"].to_numpy()\n",
    "    if len(cand_L):\n",
    "        cand_L[\"ce_n\"] = robust_minmax(cand_L[\"ce\"].to_numpy())\n",
    "        base_vals = cand.loc[cand_L.index, \"s_base\"].to_numpy()\n",
    "        cand.loc[cand_L.index, \"final\"] = GAMMA * base_vals + (1 - GAMMA) * cand_L[\"ce_n\"].to_numpy()\n",
    "\n",
    "    # 레벨링\n",
    "    base_for_levels = cand.head(min(L_CE, len(cand)))\n",
    "    p50, p75, p90 = np.percentile(base_for_levels[\"final\"].to_numpy(), [50, 75, 90])\n",
    "    def to_level(x: float) -> str:\n",
    "        if x >= p90: return \"강추\"\n",
    "        if x >= p75: return \"추천\"\n",
    "        if x >= p50: return \"참고\"\n",
    "        return \"보류\"\n",
    "    cand[\"level\"] = cand[\"final\"].apply(to_level)\n",
    "\n",
    "    # 5) Top-K\n",
    "    top = cand.sort_values(\"final\", ascending=False).head(min(topk, len(cand))).copy()\n",
    "\n",
    "    # 6) 표 생성 (추천사유는 초경량 버전 권장)\n",
    "    rows = []\n",
    "    for _, r in top.iterrows():\n",
    "        src, i = r[\"src\"], int(r[\"idx\"])\n",
    "        row = (papers_df.iloc[i] if src == \"paper\" else datasets_df.iloc[i])\n",
    "\n",
    "        # 임베딩 호출 없는 초경량 사유 (강력 추천)\n",
    "        reason = extractive_reason(\n",
    "            USER_TITLE, USER_DESC,\n",
    "            safe_text(row.get(\"title\",\"\")),\n",
    "            safe_text(row.get(\"description\",\"\")),\n",
    "            backend,\n",
    "            max_chars=100,\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"구분\": \"thesis\" if src==\"paper\" else \"dataset\",\n",
    "            \"제목\": safe_text(row.get(\"title\",\"\")),\n",
    "            \"설명\": safe_text(row.get(\"description\",\"\")),\n",
    "            \"점수\": round(float(r[\"final\"]), 4),\n",
    "            \"추천 사유\": reason,\n",
    "            \"Level\": r.get(\"level\", \"참고\"),\n",
    "            \"URL\":  safe_text(row.get(\"url\",\"\")),\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9fb5b-86ed-45f6-a137-696f2584a884",
   "metadata": {},
   "source": [
    "## 입력 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f3cf308-6fed-4900-ac4e-8681df6dc8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 질의 입력(다단계 재랭킹) ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "제목을 입력하세요:  The Effect of Self-Construal on Continued Intention of Conversational AI Agent : The Boundary Condition of Mind Perception\n",
      "설명을 입력하세요(빈칸 가능):  상호의존적 자아가 독립적 자아보다 대화형 AI의 지속 사용 의도가 높음을 시나리오 실험으로 확인됐다. 사회불안에서 고독감으로 이어지는 연쇄 매개와 마음 지각이 사용 의도를 강화하는 조절 요인임을 함께 검증한다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[논문+데이터셋 통합 TOP5 — 다단계 재랭킹]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>구분</th>\n",
       "      <th>제목</th>\n",
       "      <th>설명</th>\n",
       "      <th>점수</th>\n",
       "      <th>추천 사유</th>\n",
       "      <th>Level</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thesis</td>\n",
       "      <td>AI 스피커 이용의도에 관한 연구 : 개인의 커뮤니케이션 성향과 AI 스피커에 대한...</td>\n",
       "      <td>본 연구는 개인의 커뮤니케이션 관련 성향과 AI 스피커에 대한 인식이 AI 스피커 ...</td>\n",
       "      <td>0.8509</td>\n",
       "      <td>사회불안은 AI 스피커 이용의도와 정 적 관계를, 커뮤니케이션 회피성향은 부 적 관...</td>\n",
       "      <td>강추</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thesis</td>\n",
       "      <td>생성형 AI와 대중의 인식: 이미지 생성형 AI에 대한 설문조사 결과</td>\n",
       "      <td>본 연구의 목적은 이미지 생성형 AI(예: 미드저니, 달리)에 대한 대중의 인식, ...</td>\n",
       "      <td>0.7429</td>\n",
       "      <td>본 연구의 목적은 이미지 생성형 AI 에 대한 대중의 인식, 사용 경험 및 만족도를...</td>\n",
       "      <td>강추</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thesis</td>\n",
       "      <td>AI 수용의도가 적응 수행에 미치는 영향 : 생성형 AI 리터러시의 매개효과</td>\n",
       "      <td>본 연구의 목적은 최근 각광받고 있는 생성형 AI 리터러시에 대한 척도를 개발하는 ...</td>\n",
       "      <td>0.6359</td>\n",
       "      <td>또한, 생성형 AI 수용 의도가 높은 개인이 생성형 AI 리터러시를 통해 적응 수행...</td>\n",
       "      <td>추천</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thesis</td>\n",
       "      <td>ETRI AI 실행전략 6: 산업&amp;#183;공공 AI 활용기술 연구개발 및 적용</td>\n",
       "      <td>As the development of artificial intelligence ...</td>\n",
       "      <td>0.5578</td>\n",
       "      <td>As the development of artificial intelligence ...</td>\n",
       "      <td>추천</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thesis</td>\n",
       "      <td>The Direction of AI Classes using AI Education...</td>\n",
       "      <td>본 연구는 AI 플랫폼을 활용한 인공지능 수업에서 효과적인 내용과 방법을 제시하고자...</td>\n",
       "      <td>0.5560</td>\n",
       "      <td>AI 플랫폼을 활용한 인공지능 수업에서 효과적인 내용과 방법을 제시하고자 하였다.</td>\n",
       "      <td>추천</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       구분                                                 제목  \\\n",
       "0  thesis  AI 스피커 이용의도에 관한 연구 : 개인의 커뮤니케이션 성향과 AI 스피커에 대한...   \n",
       "1  thesis             생성형 AI와 대중의 인식: 이미지 생성형 AI에 대한 설문조사 결과   \n",
       "2  thesis         AI 수용의도가 적응 수행에 미치는 영향 : 생성형 AI 리터러시의 매개효과   \n",
       "3  thesis       ETRI AI 실행전략 6: 산업&#183;공공 AI 활용기술 연구개발 및 적용   \n",
       "4  thesis  The Direction of AI Classes using AI Education...   \n",
       "\n",
       "                                                  설명      점수  \\\n",
       "0  본 연구는 개인의 커뮤니케이션 관련 성향과 AI 스피커에 대한 인식이 AI 스피커 ...  0.8509   \n",
       "1  본 연구의 목적은 이미지 생성형 AI(예: 미드저니, 달리)에 대한 대중의 인식, ...  0.7429   \n",
       "2  본 연구의 목적은 최근 각광받고 있는 생성형 AI 리터러시에 대한 척도를 개발하는 ...  0.6359   \n",
       "3  As the development of artificial intelligence ...  0.5578   \n",
       "4  본 연구는 AI 플랫폼을 활용한 인공지능 수업에서 효과적인 내용과 방법을 제시하고자...  0.5560   \n",
       "\n",
       "                                               추천 사유 Level  \\\n",
       "0  사회불안은 AI 스피커 이용의도와 정 적 관계를, 커뮤니케이션 회피성향은 부 적 관...    강추   \n",
       "1  본 연구의 목적은 이미지 생성형 AI 에 대한 대중의 인식, 사용 경험 및 만족도를...    강추   \n",
       "2  또한, 생성형 AI 수용 의도가 높은 개인이 생성형 AI 리터러시를 통해 적응 수행...    추천   \n",
       "3  As the development of artificial intelligence ...    추천   \n",
       "4      AI 플랫폼을 활용한 인공지능 수업에서 효과적인 내용과 방법을 제시하고자 하였다.    추천   \n",
       "\n",
       "                                                 URL  \n",
       "0  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "1  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "2  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "3  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "4  http://click.ndsl.kr/servlet/OpenAPIDetailView...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "저장 완료: C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\추천_통합_다단계.csv\n"
     ]
    }
   ],
   "source": [
    "# ==== 질의 입력 & 실행 ====\n",
    "print(\"=== 질의 입력(다단계 재랭킹) ===\")\n",
    "USER_TITLE = input(\"제목을 입력하세요: \").strip()\n",
    "USER_DESC  = input(\"설명을 입력하세요(빈칸 가능): \").strip()\n",
    "if not USER_TITLE and not USER_DESC:\n",
    "    raise ValueError(\"제목/설명 중 하나는 반드시 입력해야 합니다.\")\n",
    "\n",
    "# (옵션) 간단 영문 번역문이 있으면 넣어주기. 없으면 None 유지.\n",
    "EN_TITLE = None\n",
    "EN_DESC  = None\n",
    "# 예) EN_TITLE, EN_DESC = translate(USER_TITLE), translate(USER_DESC)\n",
    "\n",
    "# 백엔드 준비(SBERT 권장)\n",
    "backend = get_backend()\n",
    "\n",
    "# 코퍼스 로드\n",
    "papers_df   = load_df(PAPERS_CSV)\n",
    "datasets_df = load_df(DATASETS_CSV)\n",
    "\n",
    "# 실행\n",
    "final_df = multistage_recommend(\n",
    "    title_ko=USER_TITLE, desc_ko=USER_DESC,\n",
    "    papers_df=papers_df, datasets_df=datasets_df,\n",
    "    backend=backend, en_title=EN_TITLE, en_desc=EN_DESC,\n",
    "    topk=max(3, min(K_FINAL, 5))\n",
    ")\n",
    "\n",
    "from IPython.display import display\n",
    "print(f\"\\n[논문+데이터셋 통합 TOP{len(final_df)} — 다단계 재랭킹]\")\n",
    "display(final_df)  # 구분, 제목, 설명, 점수, 추천 사유, URL\n",
    "\n",
    "# 저장\n",
    "final_df.to_csv(\"추천_통합_다단계.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\n저장 완료:\", os.path.abspath(\"추천_통합_다단계.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845597f9-dbee-4924-a882-977b6ca5d09d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 초기 출력 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bcb869e-b6b9-49f4-a96e-569456fc95bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 질의 입력(다단계 재랭킹) ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "제목을 입력하세요:  The Effect of Self-Construal on Continued Intention of Conversational AI Agent : The Boundary Condition of Mind Perception\n",
      "설명을 입력하세요(빈칸 가능):  As text-based chatbots diffuse into everyday life, user acceptance remains uneven. Drawing on the I-PACE framework and self-construal theory, the study examines how interdependent vs. independent self-construal influences continued intention to use conversational AI, through affective mechanisms and with boundary conditions of mind perception. In scenario experiments that primed self-construal and measured use intention and mind perception, interdependent selves exhibited higher continuance intention than independent selves. This effect operated through a serial pathway: higher social anxiety leading to greater loneliness, which in turn increased intention to use conversational AI. Moreover, mind perception strengthened the link between social anxiety and use intention. The work extends self-construal theory to the conversational-AI domain by jointly testing mediating (social anxiety, loneliness) and moderating (mind perception) processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[논문+데이터셋 통합 TOP5 — 다단계 재랭킹]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>구분</th>\n",
       "      <th>제목</th>\n",
       "      <th>설명</th>\n",
       "      <th>점수</th>\n",
       "      <th>추천 사유</th>\n",
       "      <th>Level</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thesis</td>\n",
       "      <td>AI 수용의도가 적응 수행에 미치는 영향 : 생성형 AI 리터러시의 매개효과</td>\n",
       "      <td>본 연구의 목적은 최근 각광받고 있는 생성형 AI 리터러시에 대한 척도를 개발하는 ...</td>\n",
       "      <td>0.8114</td>\n",
       "      <td>공통 키워드: ai</td>\n",
       "      <td>강추</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thesis</td>\n",
       "      <td>패션 디자인 주체에 따른 패션디자이너 역량 및 제품 품질 지각 -Human vs. ...</td>\n",
       "      <td>Collaboration between AI and fashion designers...</td>\n",
       "      <td>0.7185</td>\n",
       "      <td>공통 키워드: moderating, moreover, between, effect,...</td>\n",
       "      <td>강추</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thesis</td>\n",
       "      <td>GPT-3 기반 Human-AI Interaction의 경험 설계 : 인공지능 예술...</td>\n",
       "      <td>본 연구는 생성형 AI와 예술의 접목을 통해 인공지능과의 인터랙션에서 발생할 수 있...</td>\n",
       "      <td>0.6449</td>\n",
       "      <td>공통 키워드: ai</td>\n",
       "      <td>강추</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thesis</td>\n",
       "      <td>ETRI AI 실행전략 6: 산업&amp;#183;공공 AI 활용기술 연구개발 및 적용</td>\n",
       "      <td>As the development of artificial intelligence ...</td>\n",
       "      <td>0.5496</td>\n",
       "      <td>공통 키워드: examines, through, social, which, this...</td>\n",
       "      <td>추천</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thesis</td>\n",
       "      <td>The Direction of AI Classes using AI Education...</td>\n",
       "      <td>본 연구는 AI 플랫폼을 활용한 인공지능 수업에서 효과적인 내용과 방법을 제시하고자...</td>\n",
       "      <td>0.5245</td>\n",
       "      <td>공통 키워드: ai</td>\n",
       "      <td>추천</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       구분                                                 제목  \\\n",
       "0  thesis         AI 수용의도가 적응 수행에 미치는 영향 : 생성형 AI 리터러시의 매개효과   \n",
       "1  thesis  패션 디자인 주체에 따른 패션디자이너 역량 및 제품 품질 지각 -Human vs. ...   \n",
       "2  thesis  GPT-3 기반 Human-AI Interaction의 경험 설계 : 인공지능 예술...   \n",
       "3  thesis       ETRI AI 실행전략 6: 산업&#183;공공 AI 활용기술 연구개발 및 적용   \n",
       "4  thesis  The Direction of AI Classes using AI Education...   \n",
       "\n",
       "                                                  설명      점수  \\\n",
       "0  본 연구의 목적은 최근 각광받고 있는 생성형 AI 리터러시에 대한 척도를 개발하는 ...  0.8114   \n",
       "1  Collaboration between AI and fashion designers...  0.7185   \n",
       "2  본 연구는 생성형 AI와 예술의 접목을 통해 인공지능과의 인터랙션에서 발생할 수 있...  0.6449   \n",
       "3  As the development of artificial intelligence ...  0.5496   \n",
       "4  본 연구는 AI 플랫폼을 활용한 인공지능 수업에서 효과적인 내용과 방법을 제시하고자...  0.5245   \n",
       "\n",
       "                                               추천 사유 Level  \\\n",
       "0                                         공통 키워드: ai    강추   \n",
       "1  공통 키워드: moderating, moreover, between, effect,...    강추   \n",
       "2                                         공통 키워드: ai    강추   \n",
       "3  공통 키워드: examines, through, social, which, this...    추천   \n",
       "4                                         공통 키워드: ai    추천   \n",
       "\n",
       "                                                 URL  \n",
       "0  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "1  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "2  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "3  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "4  http://click.ndsl.kr/servlet/OpenAPIDetailView...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "저장 완료: C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\추천_통합_다단계.csv\n"
     ]
    }
   ],
   "source": [
    "# ==== 질의 입력 & 실행 ====\n",
    "print(\"=== 질의 입력(다단계 재랭킹) ===\")\n",
    "USER_TITLE = input(\"제목을 입력하세요: \").strip()\n",
    "USER_DESC  = input(\"설명을 입력하세요(빈칸 가능): \").strip()\n",
    "if not USER_TITLE and not USER_DESC:\n",
    "    raise ValueError(\"제목/설명 중 하나는 반드시 입력해야 합니다.\")\n",
    "\n",
    "# (옵션) 간단 영문 번역문이 있으면 넣어주기. 없으면 None 유지.\n",
    "EN_TITLE = None\n",
    "EN_DESC  = None\n",
    "# 예) EN_TITLE, EN_DESC = translate(USER_TITLE), translate(USER_DESC)\n",
    "\n",
    "# 백엔드 준비(SBERT 권장)\n",
    "backend = get_backend()\n",
    "\n",
    "# 코퍼스 로드\n",
    "papers_df   = load_df(PAPERS_CSV)\n",
    "datasets_df = load_df(DATASETS_CSV)\n",
    "\n",
    "# 실행\n",
    "final_df = multistage_recommend(\n",
    "    title_ko=USER_TITLE, desc_ko=USER_DESC,\n",
    "    papers_df=papers_df, datasets_df=datasets_df,\n",
    "    backend=backend, en_title=EN_TITLE, en_desc=EN_DESC,\n",
    "    topk=max(3, min(K_FINAL, 5))\n",
    ")\n",
    "\n",
    "from IPython.display import display\n",
    "print(f\"\\n[논문+데이터셋 통합 TOP{len(final_df)} — 다단계 재랭킹]\")\n",
    "display(final_df)  # 구분, 제목, 설명, 점수, 추천 사유, URL\n",
    "\n",
    "# 저장\n",
    "final_df.to_csv(\"추천_통합_다단계.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\n저장 완료:\", os.path.abspath(\"추천_통합_다단계.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77613b6-a11c-4073-9a4c-3c76795bc089",
   "metadata": {},
   "source": [
    "# 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697086c6-9648-4afb-a17b-cc918b80ce36",
   "metadata": {},
   "source": [
    "## 쿼리 샘플 뽑기 + 주석 시트 만들기(정밀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d0082c7-f556-4b78-a21d-f5e478ffabf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주석 시트 저장: C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\eval_annotation_sheet.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>q_title</th>\n",
       "      <th>q_desc</th>\n",
       "      <th>cand_id</th>\n",
       "      <th>cand_url</th>\n",
       "      <th>cand_title</th>\n",
       "      <th>cand_desc</th>\n",
       "      <th>cand_source</th>\n",
       "      <th>baseline_score</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q001</td>\n",
       "      <td>빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구</td>\n",
       "      <td>최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>기업의 빅데이터 활용 유형별 적용 방안 및 효과 분석</td>\n",
       "      <td>지난 수년 간 스마트폰과 같은 스마트 기기의 빠른 확산과 함께 인터넷과 소셜네트워크...</td>\n",
       "      <td>thesis</td>\n",
       "      <td>0.8157</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q001</td>\n",
       "      <td>빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구</td>\n",
       "      <td>최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>빅데이터 분석기법을 통한 N-스크린 서비스 활용에 영향을 미치는 요인 분석 : 로지...</td>\n",
       "      <td>스마트 디바이스의 확산으로 인해 정보통신 서비스의 다양성과 양이 크게 증가하고 있다...</td>\n",
       "      <td>thesis</td>\n",
       "      <td>0.7999</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q001</td>\n",
       "      <td>빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구</td>\n",
       "      <td>최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>빅데이터 분석을 통한 사용자 관점의 이슈 클러스터링</td>\n",
       "      <td>급증하는 빅 데이터 분석은 웹 3.0과 스마트 기기의 보편화에 대응하기 위한 기업의...</td>\n",
       "      <td>thesis</td>\n",
       "      <td>0.7736</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid                               q_title  \\\n",
       "0  Q001  빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구   \n",
       "1  Q001  빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구   \n",
       "2  Q001  빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구   \n",
       "\n",
       "                                              q_desc  \\\n",
       "0  최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...   \n",
       "1  최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...   \n",
       "2  최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...   \n",
       "\n",
       "                                             cand_id  \\\n",
       "0  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "1  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "2  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "\n",
       "                                            cand_url  \\\n",
       "0  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "1  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "2  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "\n",
       "                                          cand_title  \\\n",
       "0                      기업의 빅데이터 활용 유형별 적용 방안 및 효과 분석   \n",
       "1  빅데이터 분석기법을 통한 N-스크린 서비스 활용에 영향을 미치는 요인 분석 : 로지...   \n",
       "2                       빅데이터 분석을 통한 사용자 관점의 이슈 클러스터링   \n",
       "\n",
       "                                           cand_desc cand_source  \\\n",
       "0  지난 수년 간 스마트폰과 같은 스마트 기기의 빠른 확산과 함께 인터넷과 소셜네트워크...      thesis   \n",
       "1  스마트 디바이스의 확산으로 인해 정보통신 서비스의 다양성과 양이 크게 증가하고 있다...      thesis   \n",
       "2  급증하는 빅 데이터 분석은 웹 3.0과 스마트 기기의 보편화에 대응하기 위한 기업의...      thesis   \n",
       "\n",
       "   baseline_score rel  \n",
       "0          0.8157      \n",
       "1          0.7999      \n",
       "2          0.7736      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === A) 쿼리 샘플 뽑기 + 주석 시트 생성 ===\n",
    "# - 코퍼스에서 N_Q개 쿼리를 샘플링하고\n",
    "# - 각 쿼리별 우리 파이프라인으로 후보 TOPK_CAND를 생성해서\n",
    "#   eval_annotation_sheet.csv 를 만든다.\n",
    "\n",
    "import numpy as np, pandas as pd, hashlib, os\n",
    "\n",
    "N_Q = 50           # 평가용 쿼리 수 (50~100 권장)\n",
    "TOPK_CAND = 30     # 쿼리당 후보 수 (20~30 권장)\n",
    "ANNOT_PATH = \"eval_annotation_sheet.csv\"\n",
    "\n",
    "def safe_text(x):\n",
    "    return \"\" if pd.isna(x) else str(x)\n",
    "\n",
    "def row_to_id(src: str, title: str, url: str) -> str:\n",
    "    \"\"\"doc_id 통일: URL이 있으면 URL, 없으면 src+제목 hash\"\"\"\n",
    "    url = safe_text(url)\n",
    "    if url:\n",
    "        return url\n",
    "    h = hashlib.md5(safe_text(title).encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"{src}:{h}\"\n",
    "\n",
    "def make_queries_from_corpus(papers_df, datasets_df, n=50, seed=42):\n",
    "    # 각 코퍼스에서 절반씩 샘플. 원하면 비율 조정 가능\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_p = min(n//2, len(papers_df))\n",
    "    n_d = min(n - n_p, len(datasets_df))\n",
    "    sp = papers_df.sample(n_p, random_state=seed) if n_p>0 else papers_df.head(0)\n",
    "    sd = datasets_df.sample(n_d, random_state=seed) if n_d>0 else datasets_df.head(0)\n",
    "    rows = []\n",
    "\n",
    "    # 논문 쪽\n",
    "    for _, r in sp.iterrows():\n",
    "        rows.append({\n",
    "            \"qid\": f\"Q{len(rows)+1:03d}\",\n",
    "            \"q_title\": safe_text(r.get(\"title\",\"\")),\n",
    "            \"q_desc\":  safe_text(r.get(\"description\",\"\")),\n",
    "            \"q_src\":   \"thesis\",\n",
    "            \"q_url\":   safe_text(r.get(\"url\",\"\")),\n",
    "        })\n",
    "    # 데이터셋 쪽\n",
    "    for _, r in sd.iterrows():\n",
    "        rows.append({\n",
    "            \"qid\": f\"Q{len(rows)+1:03d}\",\n",
    "            \"q_title\": safe_text(r.get(\"title\",\"\")),\n",
    "            \"q_desc\":  safe_text(r.get(\"description\",\"\")),\n",
    "            \"q_src\":   \"dataset\",\n",
    "            \"q_url\":   safe_text(r.get(\"url\",\"\")),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_annotation_sheet(queries_df, topk_cand=30):\n",
    "    ann_rows = []\n",
    "    for _, q in queries_df.iterrows():\n",
    "        df = multistage_recommend(\n",
    "            title_ko=q[\"q_title\"], desc_ko=q[\"q_desc\"],\n",
    "            papers_df=papers_df, datasets_df=datasets_df,\n",
    "            backend=backend, en_title=None, en_desc=None,\n",
    "            topk=topk_cand\n",
    "        )\n",
    "        for _, r in df.iterrows():\n",
    "            src = safe_text(r.get(\"구분\",\"\"))  # \"thesis\"/\"dataset\"\n",
    "            title = safe_text(r.get(\"제목\",\"\") or r.get(\"title\",\"\"))\n",
    "            desc  = safe_text(r.get(\"설명\",\"\") or r.get(\"description\",\"\"))\n",
    "            url   = safe_text(r.get(\"URL\",\"\") or r.get(\"url\",\"\"))\n",
    "            doc_id = row_to_id(src, title, url)\n",
    "\n",
    "            # 자기 자신(쿼리의 원문)과 동일 URL이면 제외\n",
    "            if q[\"q_url\"] and url and (q[\"q_url\"] == url):\n",
    "                continue\n",
    "\n",
    "            ann_rows.append({\n",
    "                \"qid\": q[\"qid\"],\n",
    "                \"q_title\": q[\"q_title\"],\n",
    "                \"q_desc\":  q[\"q_desc\"],\n",
    "                \"cand_id\": doc_id,         # 평가에서 사용할 고유 id\n",
    "                \"cand_url\": url,           # 사람이 보기 편하도록 URL도 보관\n",
    "                \"cand_title\": title,\n",
    "                \"cand_desc\": desc,\n",
    "                \"cand_source\": src,        # thesis/dataset\n",
    "                \"baseline_score\": float(r.get(\"점수\", 0.0)),  # 파이프라인 점수\n",
    "                \"rel\": \"\"                  # (사람 라벨 대신 다음 셀에서 자동 채움)\n",
    "            })\n",
    "    return pd.DataFrame(ann_rows)\n",
    "\n",
    "# 실행\n",
    "queries_df = make_queries_from_corpus(papers_df, datasets_df, n=N_Q)\n",
    "ann = build_annotation_sheet(queries_df, topk_cand=TOPK_CAND)\n",
    "ann.to_csv(ANNOT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"주석 시트 저장:\", os.path.abspath(ANNOT_PATH))\n",
    "display(ann.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d179fadb-cff2-4958-80b1-f937c7a6b5f8",
   "metadata": {},
   "source": [
    "## 정밀 모드 CE/Hybrid 자동 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56278892-bcb9-47cb-aedf-601a11a1a795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자동 라벨 시트 저장: C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\eval_annotation_sheet.auto_labeled.csv\n",
      "gold 저장: C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\eval_gold.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_title</th>\n",
       "      <th>q_desc</th>\n",
       "      <th>cand_id</th>\n",
       "      <th>cand_url</th>\n",
       "      <th>cand_title</th>\n",
       "      <th>cand_desc</th>\n",
       "      <th>cand_source</th>\n",
       "      <th>baseline_score</th>\n",
       "      <th>rel</th>\n",
       "      <th>ce_score</th>\n",
       "      <th>silver_score</th>\n",
       "      <th>q_high</th>\n",
       "      <th>q_mid</th>\n",
       "      <th>qid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구</td>\n",
       "      <td>최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>기업의 빅데이터 활용 유형별 적용 방안 및 효과 분석</td>\n",
       "      <td>지난 수년 간 스마트폰과 같은 스마트 기기의 빠른 확산과 함께 인터넷과 소셜네트워크...</td>\n",
       "      <td>thesis</td>\n",
       "      <td>0.8157</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>Q001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구</td>\n",
       "      <td>최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>빅데이터 분석기법을 통한 N-스크린 서비스 활용에 영향을 미치는 요인 분석 : 로지...</td>\n",
       "      <td>스마트 디바이스의 확산으로 인해 정보통신 서비스의 다양성과 양이 크게 증가하고 있다...</td>\n",
       "      <td>thesis</td>\n",
       "      <td>0.7999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>Q001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구</td>\n",
       "      <td>최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>빅데이터 분석을 통한 사용자 관점의 이슈 클러스터링</td>\n",
       "      <td>급증하는 빅 데이터 분석은 웹 3.0과 스마트 기기의 보편화에 대응하기 위한 기업의...</td>\n",
       "      <td>thesis</td>\n",
       "      <td>0.7736</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>Q001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                q_title  \\\n",
       "0  빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구   \n",
       "1  빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구   \n",
       "2  빅데이터 기반의 로그데이터 분석을 통한 시스템 장애 감지기법 연구   \n",
       "\n",
       "                                              q_desc  \\\n",
       "0  최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...   \n",
       "1  최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...   \n",
       "2  최근 인터넷 서비스의 증가와 다양화로 인해 서비스 로그들은 기하급수적으로 증가하고 ...   \n",
       "\n",
       "                                             cand_id  \\\n",
       "0  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "1  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "2  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "\n",
       "                                            cand_url  \\\n",
       "0  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "1  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "2  http://click.ndsl.kr/servlet/OpenAPIDetailView...   \n",
       "\n",
       "                                          cand_title  \\\n",
       "0                      기업의 빅데이터 활용 유형별 적용 방안 및 효과 분석   \n",
       "1  빅데이터 분석기법을 통한 N-스크린 서비스 활용에 영향을 미치는 요인 분석 : 로지...   \n",
       "2                       빅데이터 분석을 통한 사용자 관점의 이슈 클러스터링   \n",
       "\n",
       "                                           cand_desc cand_source  \\\n",
       "0  지난 수년 간 스마트폰과 같은 스마트 기기의 빠른 확산과 함께 인터넷과 소셜네트워크...      thesis   \n",
       "1  스마트 디바이스의 확산으로 인해 정보통신 서비스의 다양성과 양이 크게 증가하고 있다...      thesis   \n",
       "2  급증하는 빅 데이터 분석은 웹 3.0과 스마트 기기의 보편화에 대응하기 위한 기업의...      thesis   \n",
       "\n",
       "   baseline_score  rel  ce_score  silver_score    q_high     q_mid   qid  \n",
       "0          0.8157    0  0.002264      0.002264  0.017025  0.005185  Q001  \n",
       "1          0.7999    0  0.000605      0.000605  0.017025  0.005185  Q001  \n",
       "2          0.7736    0  0.000691      0.000691  0.017025  0.005185  Q001  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === B) 정밀 모드 자동 라벨링 (CE / Hybrid) ===\n",
    "# - A셀에서 만든 eval_annotation_sheet.csv를 읽어서\n",
    "# - CE 또는 Hybrid 점수로 퍼센타일 컷 → rel(2/1/0) 채움\n",
    "# - eval_gold.csv 생성\n",
    "\n",
    "import os, re, numpy as np, pandas as pd, torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "ANNOT_PATH = \"eval_annotation_sheet.csv\"\n",
    "OUT_ANN    = \"eval_annotation_sheet.auto_labeled.csv\"\n",
    "OUT_GOLD   = \"eval_gold.csv\"\n",
    "\n",
    "LABEL_MODE = \"ce\"                 # \"ce\" | \"hybrid\"\n",
    "CE_MODEL   = \"BAAI/bge-reranker-v2-m3\"\n",
    "W_CE, W_BASE = 0.55, 0.45         # hybrid 가중치\n",
    "P_HIGH, P_MID = 90, 75            # p90→2, p75→1\n",
    "ABS_STRONG = None                 # 예: 0.70\n",
    "ABS_WEAK   = None                 # 예: 0.55\n",
    "\n",
    "def safe_text(x): return \"\" if pd.isna(x) else str(x)\n",
    "def robust_minmax(x, lo=5, hi=95):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if len(x)==0: return x\n",
    "    a,b = np.percentile(x, [lo,hi]); denom = max(1e-6, b-a)\n",
    "    return np.clip((x-a)/denom, 0, 1)\n",
    "\n",
    "# CE 로더 (SDPA→Eager 폴백)\n",
    "_ce_model_cache = None\n",
    "def ce_score_pairs(pairs, model_name=CE_MODEL):\n",
    "    global _ce_model_cache\n",
    "    if _ce_model_cache is None:\n",
    "        dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        def _load(model_kwargs=None):\n",
    "            return CrossEncoder(model_name, device=dev, max_length=512,\n",
    "                                **({\"model_kwargs\": model_kwargs} if model_kwargs else {}))\n",
    "        try:\n",
    "            _ce_model_cache = _load({\"attn_implementation\": \"sdpa\"})\n",
    "        except Exception:\n",
    "            _ce_model_cache = _load({\"attn_implementation\": \"eager\"})\n",
    "    return _ce_model_cache.predict(pairs)\n",
    "\n",
    "# 로드 & 점수 계산\n",
    "ann = pd.read_csv(ANNOT_PATH)\n",
    "pairs = []\n",
    "for _, r in ann.iterrows():\n",
    "    q_text = f\"{safe_text(r['q_title'])} [SEP] {safe_text(r['q_desc'])}\"\n",
    "    d_text = f\"{safe_text(r['cand_title'])} [SEP] {safe_text(r['cand_desc'])}\"\n",
    "    pairs.append((q_text, d_text))\n",
    "ce_scores = ce_score_pairs(pairs) if len(pairs) else np.array([])\n",
    "ann[\"ce_score\"] = ce_scores if len(pairs) else 0.0\n",
    "ann[\"baseline_score\"] = pd.to_numeric(ann.get(\"baseline_score\", 0.0), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "def make_silver_score(g):\n",
    "    if LABEL_MODE == \"ce\":\n",
    "        return g[\"ce_score\"].to_numpy(float)\n",
    "    elif LABEL_MODE == \"hybrid\":\n",
    "        return 0.55*robust_minmax(g[\"ce_score\"].to_numpy(float)) + \\\n",
    "               0.45*robust_minmax(g[\"baseline_score\"].to_numpy(float))\n",
    "    else:\n",
    "        raise ValueError(\"LABEL_MODE must be 'ce' or 'hybrid'.\")\n",
    "\n",
    "def assign_rel_per_query(g):\n",
    "    s = make_silver_score(g)\n",
    "    if len(s) >= 3:\n",
    "        q_high, q_mid = np.percentile(s, [P_HIGH, P_MID])\n",
    "    else:\n",
    "        q_high, q_mid = (np.max(s) if len(s) else 0.0, np.median(s) if len(s) else 0.0)\n",
    "    rel = np.zeros(len(s), dtype=int)\n",
    "    rel[s >= q_mid] = 1\n",
    "    rel[s >= q_high] = 2\n",
    "    if ABS_STRONG is not None:\n",
    "        rel[(s >= q_high) & (s < ABS_STRONG)] = 1\n",
    "    if ABS_WEAK is not None:\n",
    "        rel[(s < q_mid) & (s >= ABS_WEAK)] = 1\n",
    "    g = g.copy()\n",
    "    g[\"silver_score\"], g[\"rel\"], g[\"q_high\"], g[\"q_mid\"] = s, rel, q_high, q_mid\n",
    "    return g\n",
    "\n",
    "groups = []\n",
    "for qid, g in ann.groupby(\"qid\", sort=False):\n",
    "    gg = assign_rel_per_query(g.drop(columns=[\"qid\"], errors=\"ignore\"))  # 그룹 키 제외하고 처리\n",
    "    gg[\"qid\"] = qid                                                      # 처리 후 다시 붙이기\n",
    "    groups.append(gg)\n",
    "\n",
    "ann_lab = pd.concat(groups, ignore_index=True)\n",
    "ann_lab.to_csv(OUT_ANN, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"자동 라벨 시트 저장:\", os.path.abspath(OUT_ANN))\n",
    "\n",
    "# gold 압축 (doc_id = cand_id 사용!)\n",
    "gold_rows = []\n",
    "for qid, grp in ann_lab.groupby(\"qid\"):\n",
    "    pos = grp[grp[\"rel\"] > 0]\n",
    "    if len(pos)==0: \n",
    "        continue\n",
    "    urls = [safe_text(u) for u in pos[\"cand_id\"].tolist()]  # ← 평가에서 사용할 doc_id\n",
    "    rels = [int(x) for x in pos[\"rel\"].tolist()]\n",
    "    q_title = safe_text(grp.iloc[0][\"q_title\"])\n",
    "    q_desc  = safe_text(grp.iloc[0][\"q_desc\"])\n",
    "    gold_rows.append({\n",
    "        \"qid\": qid,\n",
    "        \"title\": q_title,\n",
    "        \"desc\": q_desc,\n",
    "        \"gold_urls\": \",\".join(urls),\n",
    "        \"gold_rels\": \",\".join(map(str, rels)),\n",
    "    })\n",
    "\n",
    "pd.DataFrame(gold_rows).to_csv(OUT_GOLD, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"gold 저장:\", os.path.abspath(OUT_GOLD))\n",
    "display(ann_lab.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3492e2-4599-4a7d-851a-7118118127e0",
   "metadata": {},
   "source": [
    "## 평가 유틸 (nDCG/MRR/Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b4ae06-3c39-4e99-a036-58df57e7b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === C) 평가 유틸 ===\n",
    "import numpy as np\n",
    "\n",
    "def dcg_at_k(rels, k=10):\n",
    "    rels = np.asfarray(rels)[:k]\n",
    "    if rels.size:\n",
    "        discounts = 1.0 / np.log2(np.arange(2, rels.size + 2))\n",
    "        return float(np.sum(rels * discounts))\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(rels, k=10):\n",
    "    dcg = dcg_at_k(rels, k)\n",
    "    idcg = dcg_at_k(sorted(rels, reverse=True), k)\n",
    "    return float(dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "def mrr_at_k(rels, k=10):\n",
    "    rels = np.asfarray(rels)[:k]\n",
    "    hits = np.where(rels > 0)[0]\n",
    "    return float(1.0 / (hits[0] + 1)) if len(hits) else 0.0\n",
    "\n",
    "def recall_at_k(rels, num_pos_total, k=10):\n",
    "    if num_pos_total <= 0: return 0.0\n",
    "    rels = np.asfarray(rels)[:k]\n",
    "    return float(np.sum(rels > 0) / num_pos_total)\n",
    "\n",
    "def evaluate(queries, retrieve_fn, gold, k=10):\n",
    "    ndcgs, mrrs, recalls = [], [], []\n",
    "    for q in queries:\n",
    "        qid = q[\"qid\"]\n",
    "        results = retrieve_fn(q, topk=k)\n",
    "        rels = [gold.get(qid, {}).get(doc_id, 0) for doc_id, _ in results]\n",
    "        num_pos = sum(1 for v in gold.get(qid, {}).values() if v > 0)\n",
    "        ndcgs.append(ndcg_at_k(rels, k))\n",
    "        mrrs.append(mrr_at_k(rels, k))\n",
    "        recalls.append(recall_at_k(rels, num_pos, k))\n",
    "    return {\n",
    "        f\"nDCG@{k}\": float(np.mean(ndcgs)) if ndcgs else 0.0,\n",
    "        f\"MRR@{k}\": float(np.mean(mrrs)) if mrrs else 0.0,\n",
    "        f\"Recall@{k}\": float(np.mean(recalls)) if recalls else 0.0,\n",
    "        \"N\": len(queries),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d817333-14be-407b-a06e-bb506126087f",
   "metadata": {},
   "source": [
    "## 평가 실행 (우리 파이프라인 ↔ gold 연결)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "989a0ab5-69e6-4077-8746-cc916c6f4cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nDCG@10': 0.6489751237263062, 'MRR@10': 0.4725555555555555, 'Recall@10': 0.5375, 'N': 50}\n"
     ]
    }
   ],
   "source": [
    "# === D) 평가 실행 ===\n",
    "import pandas as pd, hashlib\n",
    "\n",
    "# gold 로드 → queries 리스트 & gold dict 만들기\n",
    "gdf = pd.read_csv(\"eval_gold.csv\")\n",
    "queries = gdf[[\"qid\",\"title\",\"desc\"]].drop_duplicates().rename(\n",
    "    columns={\"title\":\"title\",\"desc\":\"desc\"}\n",
    ").to_dict(\"records\")\n",
    "\n",
    "# gold dict: {qid: {doc_id: rel}}\n",
    "gold = {}\n",
    "for _, r in gdf.iterrows():\n",
    "    urls = [u.strip() for u in str(r[\"gold_urls\"]).split(\",\") if u.strip()]\n",
    "    rels = [int(x) for x in str(r[\"gold_rels\"]).split(\",\") if str(x).strip().isdigit()]\n",
    "    if rels and len(rels) != len(urls):\n",
    "        rels = [1]*len(urls)\n",
    "    if not rels:\n",
    "        rels = [1]*len(urls)\n",
    "    gold[r[\"qid\"]] = {u: rels[i] for i,u in enumerate(urls)}\n",
    "\n",
    "# doc_id 생성 로직을 annotation 시트와 동일하게 맞춘다\n",
    "def row_to_id(src: str, title: str, url: str) -> str:\n",
    "    url = str(url or \"\")\n",
    "    if url:\n",
    "        return url\n",
    "    h = hashlib.md5(str(title or \"\").encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"{src}:{h}\"\n",
    "\n",
    "# 우리 파이프라인 호출 → doc_id 리스트 반환\n",
    "def retrieve_full(q, topk=10):\n",
    "    df = multistage_recommend(\n",
    "        title_ko=q[\"title\"], desc_ko=q[\"desc\"],\n",
    "        papers_df=papers_df, datasets_df=datasets_df,\n",
    "        backend=backend, en_title=None, en_desc=None, topk=topk\n",
    "    )\n",
    "    out = []\n",
    "    for _, r in df.iterrows():\n",
    "        src   = str(r.get(\"구분\",\"\"))\n",
    "        title = str(r.get(\"제목\",\"\") or r.get(\"title\",\"\"))\n",
    "        url   = str(r.get(\"URL\",\"\") or r.get(\"url\",\"\"))\n",
    "        doc_id = row_to_id(src, title, url)\n",
    "        out.append((doc_id, float(r.get(\"점수\", 0.0))))\n",
    "    return out\n",
    "\n",
    "report = evaluate(\n",
    "    [{\"qid\":q[\"qid\"], \"title\":q[\"title\"], \"desc\":q[\"desc\"]} for q in queries],\n",
    "    retrieve_full, gold, k=10\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd394a3b-064c-43a5-8375-24aa3908d554",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 빠른 평가용(간소화) - but 평가 성능 떨어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "867d84db-5a7a-4b23-bd00-063151695622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cache once ===\n",
    "try:\n",
    "    bm25_p, bm25_d, vecs_p, vecs_d\n",
    "except NameError:\n",
    "    bm25_p = WeightedBM25(papers_df,   FIELD_WEIGHTS)\n",
    "    bm25_d = WeightedBM25(datasets_df, FIELD_WEIGHTS)\n",
    "    texts_p = [compose_dense_text(r) for _, r in papers_df.iterrows()]\n",
    "    texts_d = [compose_dense_text(r) for _, r in datasets_df.iterrows()]\n",
    "    vecs_p  = backend.encode(texts_p)   # normalize=True → 코사인=내적\n",
    "    vecs_d  = backend.encode(texts_d)\n",
    "\n",
    "def _robust_minmax(x, lo=5, hi=95):\n",
    "    import numpy as np\n",
    "    a,b = np.percentile(x, [lo,hi]); denom = max(1e-6, b-a)\n",
    "    return np.clip((x-a)/denom, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623a6923-f798-4b23-a97a-2ac5e4394810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, hashlib\n",
    "\n",
    "def row_to_id(src: str, title: str, url: str) -> str:\n",
    "    url = str(url or \"\")\n",
    "    if url: return url\n",
    "    h = hashlib.md5(str(title or \"\").encode(\"utf-8\")).hexdigest()[:10]\n",
    "    return f\"{src}:{h}\"\n",
    "\n",
    "# 2-1) BM25-only\n",
    "def retrieve_bm25_only(q, topk=10):\n",
    "    q_ko = f\"{q['title']} {q['desc']}\".strip()\n",
    "    toks = lite_tokens(q_ko)\n",
    "    b_p = bm25_p.score(toks); b_d = bm25_d.score(toks)\n",
    "\n",
    "    idx_p = np.argpartition(-b_p, min(TOPN_BM25, len(b_p))-1)[:min(TOPN_BM25, len(b_p))]\n",
    "    idx_d = np.argpartition(-b_d, min(TOPN_BM25, len(b_d))-1)[:min(TOPN_BM25, len(b_d))]\n",
    "    cand_p = pd.DataFrame({\"src\":\"thesis\",\"idx\":idx_p, \"bm25\":b_p[idx_p]})\n",
    "    cand_d = pd.DataFrame({\"src\":\"dataset\",\"idx\":idx_d, \"bm25\":b_d[idx_d]})\n",
    "    cand   = pd.concat([cand_p, cand_d], ignore_index=True)\n",
    "\n",
    "    cand[\"s\"] = _robust_minmax(cand[\"bm25\"].to_numpy())  # 점수는 BM25 정규화\n",
    "    cand = cand.sort_values(\"s\", ascending=False).head(topk)\n",
    "\n",
    "    out = []\n",
    "    for _, r in cand.iterrows():\n",
    "        i, src = int(r[\"idx\"]), r[\"src\"]\n",
    "        row = (papers_df.iloc[i] if src==\"thesis\" else datasets_df.iloc[i])\n",
    "        out.append((row_to_id(src, row.get(\"title\",\"\"), row.get(\"url\",\"\")), float(r[\"s\"])))\n",
    "    return out\n",
    "\n",
    "# 2-2) Dense-only (전 문서 벡터와 내적 → 상위 선별)\n",
    "def retrieve_dense_only(q, topk=10, N_DENSE_POOL=500):\n",
    "    q_ko = f\"{q['title']} {q['desc']}\".strip()\n",
    "    q_vec = combine_query_vec(backend, q_ko, None)\n",
    "\n",
    "    s_p = vecs_p @ q_vec\n",
    "    s_d = vecs_d @ q_vec\n",
    "    kp  = min(N_DENSE_POOL, len(s_p))\n",
    "    kd  = min(N_DENSE_POOL, len(s_d))\n",
    "    idx_p = np.argpartition(-s_p, kp-1)[:kp]\n",
    "    idx_d = np.argpartition(-s_d, kd-1)[:kd]\n",
    "    cand_p = pd.DataFrame({\"src\":\"thesis\",\"idx\":idx_p, \"dense\":s_p[idx_p]})\n",
    "    cand_d = pd.DataFrame({\"src\":\"dataset\",\"idx\":idx_d, \"dense\":s_d[idx_d]})\n",
    "    cand   = pd.concat([cand_p, cand_d], ignore_index=True)\n",
    "\n",
    "    cand[\"s\"] = _robust_minmax(cand[\"dense\"].to_numpy())\n",
    "    cand = cand.sort_values(\"s\", ascending=False).head(topk)\n",
    "    out = []\n",
    "    for _, r in cand.iterrows():\n",
    "        i, src = int(r[\"idx\"]), r[\"src\"]\n",
    "        row = (papers_df.iloc[i] if src==\"thesis\" else datasets_df.iloc[i])\n",
    "        out.append((row_to_id(src, row.get(\"title\",\"\"), row.get(\"url\",\"\")), float(r[\"s\"])))\n",
    "    return out\n",
    "\n",
    "# 2-3) BM25→Dense (현재 “빠른” 기본형: s_base = α*BM25n + β*Densen)\n",
    "def retrieve_bm25_dense(q, topk=10):\n",
    "    q_ko = f\"{q['title']} {q['desc']}\".strip()\n",
    "    toks = lite_tokens(q_ko)\n",
    "    b_p = bm25_p.score(toks); b_d = bm25_d.score(toks)\n",
    "\n",
    "    kbp = min(TOPN_BM25, len(b_p)); kbd = min(TOPN_BM25, len(b_d))\n",
    "    idx_p = np.argpartition(-b_p, kbp-1)[:kbp]\n",
    "    idx_d = np.argpartition(-b_d, kbd-1)[:kbd]\n",
    "\n",
    "    q_vec = combine_query_vec(backend, q_ko, None)\n",
    "    s_p = vecs_p[idx_p] @ q_vec\n",
    "    s_d = vecs_d[idx_d] @ q_vec\n",
    "\n",
    "    cand_p = pd.DataFrame({\"src\":\"thesis\",\"idx\":idx_p, \"bm25\":b_p[idx_p], \"dense\":s_p})\n",
    "    cand_d = pd.DataFrame({\"src\":\"dataset\",\"idx\":idx_d, \"bm25\":b_d[idx_d], \"dense\":s_d})\n",
    "    cand   = pd.concat([cand_p, cand_d], ignore_index=True)\n",
    "    cand[\"bm25_n\"]  = _robust_minmax(cand[\"bm25\"].to_numpy())\n",
    "    cand[\"dense_n\"] = _robust_minmax(cand[\"dense\"].to_numpy())\n",
    "    cand[\"s_base\"]  = ALPHA * cand[\"bm25_n\"] + BETA * cand[\"dense_n\"]\n",
    "\n",
    "    cand = cand.sort_values(\"s_base\", ascending=False).head(topk)\n",
    "    out = []\n",
    "    for _, r in cand.iterrows():\n",
    "        i, src = int(r[\"idx\"]), r[\"src\"]\n",
    "        row = (papers_df.iloc[i] if src==\"thesis\" else datasets_df.iloc[i])\n",
    "        out.append((row_to_id(src, row.get(\"title\",\"\"), row.get(\"url\",\"\")), float(r[\"s_base\"])))\n",
    "    return out\n",
    "\n",
    "# 2-4) BM25+Dense+CE (정밀, 느림: 작은 샘플에만)\n",
    "from sentence_transformers import CrossEncoder\n",
    "_ce_model = None\n",
    "def retrieve_bm25_dense_ce(q, topk=10, L_CE_eval=30, CE_MODEL=\"BAAI/bge-reranker-v2-m3\"):\n",
    "    global _ce_model\n",
    "    # 후보는 bm25_dense로 만들고 상위 L_CE_eval만 CE 재정렬\n",
    "    base = retrieve_bm25_dense(q, topk=L_CE_eval)\n",
    "\n",
    "    if _ce_model is None:\n",
    "        import torch\n",
    "        dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        def _load(kwargs=None):\n",
    "            return CrossEncoder(CE_MODEL, device=dev, max_length=512,\n",
    "                                **({\"model_kwargs\": kwargs} if kwargs else {}))\n",
    "        try:\n",
    "            _ce_model = _load({\"attn_implementation\":\"sdpa\"})\n",
    "        except Exception:\n",
    "            _ce_model = _load({\"attn_implementation\":\"eager\"})\n",
    "\n",
    "    # CE 입력쌍 만들기\n",
    "    q_text = f\"{q['title']} [SEP] {q['desc']}\"\n",
    "    pairs, rows = [], []\n",
    "    for doc_id, _ in base:\n",
    "        # doc_id를 다시 row로 역참조(빠르게 하려면 doc_id→row 인덱스 캐시 추천)\n",
    "        # 여기선 간단히 URL 매칭으로 시도\n",
    "        src, title_or_hash = doc_id.split(\":\", 1) if \"://\" not in doc_id else (None, None)\n",
    "        # 안전한 버전: base 후보를 다시 multistage 없이 제목/URL로 찾아 텍스트 구성\n",
    "        # 실용상 base 점수만으로도 상위는 잘 들어오므로 간단화:\n",
    "        # (생략) → pairs에는 title/desc가 필요하므로 여기선 retrieve_bm25_dense를 좀 수정해야 하지만\n",
    "        # 너가 이미 multistage_recommend 결과 프레임을 가지고 있다면 그걸 재사용하는 편이 더 깔끔해.\n",
    "        pass  # ★ CE 정밀평가는 느리므로 소수 쿼리에서 multistage_recommend로 돌리는 편을 권장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb4d1cc5-6b4c-4419-aaca-939b4d6f1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_union_bm25_dense(q, topk=10, N_BM25=400, N_DENSE=400):\n",
    "    q_ko = f\"{q['title']} {q['desc']}\".strip()\n",
    "    toks = lite_tokens(q_ko)\n",
    "    b_p = bm25_p.score(toks); b_d = bm25_d.score(toks)\n",
    "\n",
    "    kbp = min(N_BM25, len(b_p)); kbd = min(N_BM25, len(b_d))\n",
    "    idx_bp = np.argpartition(-b_p, kbp-1)[:kbp]\n",
    "    idx_bd = np.argpartition(-b_d, kbd-1)[:kbd]\n",
    "\n",
    "    q_vec = combine_query_vec(backend, q_ko, None)\n",
    "    s_all_p = vecs_p @ q_vec\n",
    "    s_all_d = vecs_d @ q_vec\n",
    "    kdp = min(N_DENSE, len(s_all_p)); kdd = min(N_DENSE, len(s_all_d))\n",
    "    idx_dp = np.argpartition(-s_all_p, kdp-1)[:kdp]\n",
    "    idx_dd = np.argpartition(-s_all_d, kdd-1)[:kdd]\n",
    "\n",
    "    # 합집합 후보\n",
    "    idx_p = np.unique(np.concatenate([idx_bp, idx_dp]))\n",
    "    idx_d = np.unique(np.concatenate([idx_bd, idx_dd]))\n",
    "\n",
    "    # 점수 계산\n",
    "    cand_p = pd.DataFrame({\n",
    "        \"src\":\"thesis\",\"idx\":idx_p,\n",
    "        \"bm25\":b_p[idx_p], \"dense\":s_all_p[idx_p]\n",
    "    })\n",
    "    cand_d = pd.DataFrame({\n",
    "        \"src\":\"dataset\",\"idx\":idx_d,\n",
    "        \"bm25\":b_d[idx_d], \"dense\":s_all_d[idx_d]\n",
    "    })\n",
    "    cand = pd.concat([cand_p, cand_d], ignore_index=True)\n",
    "    cand[\"bm25_n\"]  = _robust_minmax(cand[\"bm25\"].to_numpy())\n",
    "    cand[\"dense_n\"] = _robust_minmax(cand[\"dense\"].to_numpy())\n",
    "    cand[\"s_base\"]  = ALPHA * cand[\"bm25_n\"] + BETA * cand[\"dense_n\"]\n",
    "\n",
    "    cand = cand.sort_values(\"s_base\", ascending=False).head(topk)\n",
    "\n",
    "    out = []\n",
    "    for _, r in cand.iterrows():\n",
    "        i, src = int(r[\"idx\"]), r[\"src\"]\n",
    "        row = (papers_df.iloc[i] if src==\"thesis\" else datasets_df.iloc[i])\n",
    "        out.append((row_to_id(src, row.get(\"title\",\"\"), row.get(\"url\",\"\")), float(r[\"s_base\"])))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2757ae7e-7254-4101-991d-eb7a459e102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold 로드 → queries 리스트 & gold dict 만들기\n",
    "gdf = pd.read_csv(\"eval_gold.csv\")\n",
    "queries = gdf[[\"qid\",\"title\",\"desc\"]].drop_duplicates().rename(\n",
    "    columns={\"title\":\"title\",\"desc\":\"desc\"}\n",
    ").to_dict(\"records\")\n",
    "\n",
    "# gold dict: {qid: {doc_id: rel}}\n",
    "gold = {}\n",
    "for _, r in gdf.iterrows():\n",
    "    urls = [u.strip() for u in str(r[\"gold_urls\"]).split(\",\") if u.strip()]\n",
    "    rels = [int(x) for x in str(r[\"gold_rels\"]).split(\",\") if str(x).strip().isdigit()]\n",
    "    if rels and len(rels) != len(urls):\n",
    "        rels = [1]*len(urls)\n",
    "    if not rels:\n",
    "        rels = [1]*len(urls)\n",
    "    gold[r[\"qid\"]] = {u: rels[i] for i,u in enumerate(urls)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5b92ef9-dec5-4e63-b457-faaced06b96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 {'nDCG@10': 0.47664260661637575, 'MRR@10': 0.4027698412698413, 'Recall@10': 0.245, 'N': 50}\n",
      "Dense {'nDCG@10': 0.4205274025639276, 'MRR@10': 0.3129047619047619, 'Recall@10': 0.2075, 'N': 50}\n",
      "BM25+Dense {'nDCG@10': 0.5987783055813736, 'MRR@10': 0.524079365079365, 'Recall@10': 0.41, 'N': 50}\n",
      "Recall++ (Union) {'nDCG@10': 0.5253263317774505, 'MRR@10': 0.4214126984126984, 'Recall@10': 0.3375, 'N': 50}\n"
     ]
    }
   ],
   "source": [
    "runs = [\n",
    "    (\"BM25\", retrieve_bm25_only),\n",
    "    (\"Dense\", retrieve_dense_only),\n",
    "    (\"BM25+Dense\", retrieve_bm25_dense),\n",
    "    (\"Recall++ (Union)\", retrieve_union_bm25_dense),\n",
    "    # (\"BM25+Dense+CE (slow sample)\", retrieve_bm25_dense_ce),  # 샘플 소수에만 권장\n",
    "]\n",
    "for name, fn in runs:\n",
    "    rep = evaluate(\n",
    "        [{\"qid\":q[\"qid\"], \"title\":q[\"title\"], \"desc\":q[\"desc\"]} for q in queries],\n",
    "        fn, gold, k=10\n",
    "    )\n",
    "    print(name, rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c6a8b-66d7-4518-9da9-4a4b0132a857",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 첫번째 수정 출력 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee89e4f0-2bce-4fc0-a839-74afb8639654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 질의 입력 ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "제목을 입력하세요:  코로나19 이후 소비트렌드 변화에 따른 지식재산권 관련 이슈 및 시사점 \n",
      "설명을 입력하세요(빈칸 가능):  코로나19는 전 세계적으로 사람들의 활동 방식과 경제 구조에 급격한 변화를 불러왔다. 비대면과 온라인 활동의 증가는 소비 트렌드를 오프라인 중심에서 온라인 중심으로 이동시켰고, 이에 따라 소비자 행동에도 두 가지 주요 변화가 나타났다. 사회적 거리두기로 온라인 소비가 가속화되었으며, 사회경제적 불확실성의 확대는 건강과 위생에 대한 관심을 높였다. 이러한 온라인 소비 방식의 전환은 지식재산권과 관련된 다양한 이슈를 발생시켰고, 결과적으로 코로나19 이후 소비 트렌드는 비대면과 재택 중심으로 재편되며 온라인 쇼핑이 급격히 확대되었다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[논문+데이터셋 통합 TOP5]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>구분</th>\n",
       "      <th>제목</th>\n",
       "      <th>설명</th>\n",
       "      <th>점수</th>\n",
       "      <th>추천 사유</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paper</td>\n",
       "      <td>포스트 코로나, 코로나 이후 온라인 미술시장</td>\n",
       "      <td>코로나 팬데믹 이후, 전염병의 창궐 속에서 개인의 건강과 안녕을 위한 수단으로 '언...</td>\n",
       "      <td>0.7774</td>\n",
       "      <td>공통 키워드: 사람들의, 불러왔다, 중심으로, 변화를, 온라인, 건강과</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paper</td>\n",
       "      <td>코로나19 이후 소비.미디어 이용행태 변화와 코로나 우울에 관한 연구</td>\n",
       "      <td>본 연구결과를 요약하면 다음과 같다. &amp;amp;#xD; 첫째, 코로나19이후 소비행...</td>\n",
       "      <td>0.7457</td>\n",
       "      <td>공통 키워드: 코로나19, 나타났다, 변화가, 따라, 소비, 따른</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paper</td>\n",
       "      <td>농어촌 정보화의 포스트 코로나 대응 변화에 대한 사례 연구: 해외 농어촌 정보화 정...</td>\n",
       "      <td>2019년 12월부터 진행된 코로나19(Covid-19)의 팬데믹 상황이 지속되면서...</td>\n",
       "      <td>0.6926</td>\n",
       "      <td>공통 키워드: 코로나19, 중심으로, 변화에, 사회적, 이러한, 이에</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paper</td>\n",
       "      <td>'코로나-19 : 우리의 기억' : 코로나바이러스 감염증과 사회변화에 대한 디지털 ...</td>\n",
       "      <td>코로나바이러스감염증은 인류사회가 경험하지 못한 커다란 충격과 생활양식의 급속한 변화...</td>\n",
       "      <td>0.6812</td>\n",
       "      <td>공통 키워드: 변화를, 사회적, 다양한, 이슈를, 주요</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paper</td>\n",
       "      <td>포스트 코로나 시대의 신유통(新零售)에 관한 검토</td>\n",
       "      <td>With the spread of Corona19, social distancing...</td>\n",
       "      <td>0.6612</td>\n",
       "      <td>주요 키워드가 부분적으로 유사합니다.</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      구분                                                 제목  \\\n",
       "0  paper                           포스트 코로나, 코로나 이후 온라인 미술시장   \n",
       "1  paper             코로나19 이후 소비.미디어 이용행태 변화와 코로나 우울에 관한 연구   \n",
       "2  paper  농어촌 정보화의 포스트 코로나 대응 변화에 대한 사례 연구: 해외 농어촌 정보화 정...   \n",
       "3  paper  '코로나-19 : 우리의 기억' : 코로나바이러스 감염증과 사회변화에 대한 디지털 ...   \n",
       "4  paper                        포스트 코로나 시대의 신유통(新零售)에 관한 검토   \n",
       "\n",
       "                                                  설명      점수  \\\n",
       "0  코로나 팬데믹 이후, 전염병의 창궐 속에서 개인의 건강과 안녕을 위한 수단으로 '언...  0.7774   \n",
       "1  본 연구결과를 요약하면 다음과 같다. &amp;#xD; 첫째, 코로나19이후 소비행...  0.7457   \n",
       "2  2019년 12월부터 진행된 코로나19(Covid-19)의 팬데믹 상황이 지속되면서...  0.6926   \n",
       "3  코로나바이러스감염증은 인류사회가 경험하지 못한 커다란 충격과 생활양식의 급속한 변화...  0.6812   \n",
       "4  With the spread of Corona19, social distancing...  0.6612   \n",
       "\n",
       "                                     추천 사유  \\\n",
       "0  공통 키워드: 사람들의, 불러왔다, 중심으로, 변화를, 온라인, 건강과   \n",
       "1     공통 키워드: 코로나19, 나타났다, 변화가, 따라, 소비, 따른   \n",
       "2   공통 키워드: 코로나19, 중심으로, 변화에, 사회적, 이러한, 이에   \n",
       "3           공통 키워드: 변화를, 사회적, 다양한, 이슈를, 주요   \n",
       "4                     주요 키워드가 부분적으로 유사합니다.   \n",
       "\n",
       "                                                 URL  \n",
       "0  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "1  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "2  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "3  http://click.ndsl.kr/servlet/OpenAPIDetailView...  \n",
       "4  http://click.ndsl.kr/servlet/OpenAPIDetailView...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 저장 완료:\n",
      " - 개별(논문): C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\recommendations.papers.csv\n",
      " - 개별(데이터셋): C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\recommendations.datasets.csv\n",
      " - 통합(영문 헤더): C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\recommendations.merged.csv\n",
      " - 통합(한글 헤더): C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\추천_통합_결과.csv\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# === 통합 추천(논문+데이터셋, 3~5건) + '구분' 열 포함 출력 ===\n",
    "# - 기존: 논문 5건, 데이터셋 5건 별도 추천 및 저장\n",
    "# - 변경: 두 코퍼스에서 점수 기준으로 상위 N(3~5)개를 통합 추천 + 첫 열 '구분' 추가\n",
    "\n",
    "TOPK_TOTAL = 5  # ← 3~5 사이에서 설정 (원하면 3 또는 4로 바꾸기)\n",
    "\n",
    "# ===== 질의 입력 =====\n",
    "print(\"=== 질의 입력 ===\")\n",
    "USER_TITLE = input(\"제목을 입력하세요: \").strip()\n",
    "USER_DESC  = input(\"설명을 입력하세요(빈칸 가능): \").strip()\n",
    "if not USER_TITLE and not USER_DESC:\n",
    "    raise ValueError(\"제목/설명 중 하나는 반드시 입력해야 합니다.\")\n",
    "\n",
    "# ===== 백엔드 1회 생성 (SBERT/TF-IDF) =====\n",
    "backend = get_backend()\n",
    "\n",
    "# ===== 논문 코퍼스 =====\n",
    "papers_df  = load_df(PAPERS_CSV)\n",
    "papers_top = run_recommendation_for_corpus(papers_df, backend, USER_TITLE, USER_DESC, TOPK)\n",
    "papers_top[\"type\"] = \"paper\"     # 구분\n",
    "\n",
    "# ===== 데이터셋 코퍼스 =====\n",
    "datasets_df  = load_df(DATASETS_CSV)\n",
    "datasets_top = run_recommendation_for_corpus(datasets_df, backend, USER_TITLE, USER_DESC, TOPK)\n",
    "datasets_top[\"type\"] = \"dataset\" # 구분\n",
    "\n",
    "# ===== 통합 머지 & 상위 N 선택 =====\n",
    "merged = (\n",
    "    pd.concat([papers_top, datasets_top], ignore_index=True)\n",
    "    .sort_values(\"score\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 안전하게 3~5로 클램핑\n",
    "N = int(max(3, min(int(TOPK_TOTAL), 5)))\n",
    "merged_top = merged.head(N).copy()\n",
    "\n",
    "# ===== 저장(영문 헤더) =====\n",
    "MERGED_CSV = \"recommendations.merged.csv\"\n",
    "merged_top.to_csv(MERGED_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# (옵션) 한국어 헤더 버전도 저장\n",
    "MERGED_CSV_KO = \"추천_통합_결과.csv\"\n",
    "save_cols_ko = {\n",
    "    \"type\": \"구분\",\n",
    "    \"title\": \"제목\",\n",
    "    \"description\": \"설명\",\n",
    "    \"score\": \"점수\",\n",
    "    \"reason\": \"추천 사유\",\n",
    "    \"url\": \"URL\",\n",
    "}\n",
    "merged_top_ko = merged_top.rename(columns=save_cols_ko)[list(save_cols_ko.values())]\n",
    "merged_top_ko.to_csv(MERGED_CSV_KO, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ===== 기존 개별 CSV도 유지(원하면 이 두 줄은 지워도 됨) =====\n",
    "papers_top.to_csv(OUTPUT_PAPERS, index=False, encoding=\"utf-8-sig\")\n",
    "datasets_top.to_csv(OUTPUT_DATA, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ===== 표시(요구 포맷: 첫 열에 '구분') =====\n",
    "from IPython.display import display\n",
    "\n",
    "display_cols = [\"type\", \"title\", \"description\", \"score\", \"reason\", \"url\"]\n",
    "view = merged_top[display_cols].rename(columns={\n",
    "    \"type\": \"구분\",\n",
    "    \"title\": \"제목\",\n",
    "    \"description\": \"설명\",\n",
    "    \"score\": \"점수\",\n",
    "    \"reason\": \"추천 사유\",\n",
    "    \"url\": \"URL\",\n",
    "})\n",
    "print(f\"\\n[논문+데이터셋 통합 TOP{N}]\")\n",
    "display(view)\n",
    "\n",
    "import os\n",
    "print(\"\\n 저장 완료:\")\n",
    "print(\" - 개별(논문):\", os.path.abspath(OUTPUT_PAPERS))\n",
    "print(\" - 개별(데이터셋):\", os.path.abspath(OUTPUT_DATA))\n",
    "print(\" - 통합(영문 헤더):\", os.path.abspath(MERGED_CSV))\n",
    "print(\" - 통합(한글 헤더):\", os.path.abspath(MERGED_CSV_KO))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9bf3d-8320-44c8-8bcd-9252bfa85d7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 수정 전 초기출력 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "974bbb5d-df36-40f7-8e6b-d220e77265e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 질의 입력 ===\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "제목을 입력하세요:  코로나19 이후 소비트렌드 변화에 따른 지식재산권 관련 이슈 및 시사점 \n",
      "설명을 입력하세요(빈칸 가능):  코로나19는 전 세계적으로 사람들의 활동 방식과 경제 구조에 급격한 변화를 불러왔다. 비대면과 온라인 활동의 증가는 소비 트렌드를 오프라인 중심에서 온라인 중심으로 이동시켰고, 이에 따라 소비자 행동에도 두 가지 주요 변화가 나타났다. 사회적 거리두기로 온라인 소비가 가속화되었으며, 사회경제적 불확실성의 확대는 건강과 위생에 대한 관심을 높였다. 이러한 온라인 소비 방식의 전환은 지식재산권과 관련된 다양한 이슈를 발생시켰고, 결과적으로 코로나19 이후 소비 트렌드는 비대면과 재택 중심으로 재편되며 온라인 쇼핑이 급격히 확대되었다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[논문 추천 TOP5]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>reason</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>포스트 코로나, 코로나 이후 온라인 미술시장</td>\n",
       "      <td>코로나 팬데믹 이후, 전염병의 창궐 속에서 개인의 건강과 안녕을 위한 수단으로 '언...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>0.7774</td>\n",
       "      <td>공통 키워드: 중심으로, 불러왔다, 사람들의, 건강과, 변화를, 방식과</td>\n",
       "      <td>강추</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>코로나19 이후 소비.미디어 이용행태 변화와 코로나 우울에 관한 연구</td>\n",
       "      <td>본 연구결과를 요약하면 다음과 같다. &amp;amp;#xD; 첫째, 코로나19이후 소비행...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>0.7457</td>\n",
       "      <td>공통 키워드: 코로나19, 나타났다, 변화가, 따라, 소비, 따른</td>\n",
       "      <td>추천</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>농어촌 정보화의 포스트 코로나 대응 변화에 대한 사례 연구: 해외 농어촌 정보화 정...</td>\n",
       "      <td>2019년 12월부터 진행된 코로나19(Covid-19)의 팬데믹 상황이 지속되면서...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>0.6926</td>\n",
       "      <td>공통 키워드: 코로나19, 중심으로, 사회적, 변화에, 이러한, 따라</td>\n",
       "      <td>참고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>'코로나-19 : 우리의 기억' : 코로나바이러스 감염증과 사회변화에 대한 디지털 ...</td>\n",
       "      <td>코로나바이러스감염증은 인류사회가 경험하지 못한 커다란 충격과 생활양식의 급속한 변화...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>0.6812</td>\n",
       "      <td>공통 키워드: 이슈를, 사회적, 다양한, 변화를, 주요</td>\n",
       "      <td>참고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>포스트 코로나 시대의 신유통(新零售)에 관한 검토</td>\n",
       "      <td>With the spread of Corona19, social distancing...</td>\n",
       "      <td>http://click.ndsl.kr/servlet/OpenAPIDetailView...</td>\n",
       "      <td>0.6612</td>\n",
       "      <td>주요 키워드가 부분적으로 유사합니다.</td>\n",
       "      <td>참고</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                                              title  \\\n",
       "0     1                           포스트 코로나, 코로나 이후 온라인 미술시장   \n",
       "1     2             코로나19 이후 소비.미디어 이용행태 변화와 코로나 우울에 관한 연구   \n",
       "2     3  농어촌 정보화의 포스트 코로나 대응 변화에 대한 사례 연구: 해외 농어촌 정보화 정...   \n",
       "3     4  '코로나-19 : 우리의 기억' : 코로나바이러스 감염증과 사회변화에 대한 디지털 ...   \n",
       "4     5                        포스트 코로나 시대의 신유통(新零售)에 관한 검토   \n",
       "\n",
       "                                         description  \\\n",
       "0  코로나 팬데믹 이후, 전염병의 창궐 속에서 개인의 건강과 안녕을 위한 수단으로 '언...   \n",
       "1  본 연구결과를 요약하면 다음과 같다. &amp;#xD; 첫째, 코로나19이후 소비행...   \n",
       "2  2019년 12월부터 진행된 코로나19(Covid-19)의 팬데믹 상황이 지속되면서...   \n",
       "3  코로나바이러스감염증은 인류사회가 경험하지 못한 커다란 충격과 생활양식의 급속한 변화...   \n",
       "4  With the spread of Corona19, social distancing...   \n",
       "\n",
       "                                                 url   score  \\\n",
       "0  http://click.ndsl.kr/servlet/OpenAPIDetailView...  0.7774   \n",
       "1  http://click.ndsl.kr/servlet/OpenAPIDetailView...  0.7457   \n",
       "2  http://click.ndsl.kr/servlet/OpenAPIDetailView...  0.6926   \n",
       "3  http://click.ndsl.kr/servlet/OpenAPIDetailView...  0.6812   \n",
       "4  http://click.ndsl.kr/servlet/OpenAPIDetailView...  0.6612   \n",
       "\n",
       "                                    reason level  \n",
       "0  공통 키워드: 중심으로, 불러왔다, 사람들의, 건강과, 변화를, 방식과    강추  \n",
       "1     공통 키워드: 코로나19, 나타났다, 변화가, 따라, 소비, 따른    추천  \n",
       "2   공통 키워드: 코로나19, 중심으로, 사회적, 변화에, 이러한, 따라    참고  \n",
       "3           공통 키워드: 이슈를, 사회적, 다양한, 변화를, 주요    참고  \n",
       "4                     주요 키워드가 부분적으로 유사합니다.    참고  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[데이터셋 추천 TOP5]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>score</th>\n",
       "      <th>reason</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Replication Data for: Placebo-Augmented PICA D...</td>\n",
       "      <td>Most media experiments analyze the impact of m...</td>\n",
       "      <td>https://doi.org/10.7910/DVN/1KQN5U</td>\n",
       "      <td>0.4522</td>\n",
       "      <td>주요 키워드가 부분적으로 유사합니다.</td>\n",
       "      <td>강추</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Local Fairs</td>\n",
       "      <td>Supported by funding from the Department of Ar...</td>\n",
       "      <td>https://dx.doi.org/10.7925/drs1.duchas_5191737</td>\n",
       "      <td>0.3778</td>\n",
       "      <td>주요 키워드가 부분적으로 유사합니다.</td>\n",
       "      <td>참고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Escolar - Annual Distribution</td>\n",
       "      <td>&lt;a href='https://maps.mpi.govt.nz/templates/MP...</td>\n",
       "      <td>https://catalogue.data.govt.nz/dataset/b6cd615...</td>\n",
       "      <td>0.3607</td>\n",
       "      <td>주요 키워드가 부분적으로 유사합니다.</td>\n",
       "      <td>참고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Action to beat Coronavirus Study [Ab-C, study ...</td>\n",
       "      <td>&lt;b&gt;Background:&lt;/b&gt; In the early months of the ...</td>\n",
       "      <td>https://dx.doi.org/10.5683/SP3/LA2IKO</td>\n",
       "      <td>0.3524</td>\n",
       "      <td>주요 키워드가 부분적으로 유사합니다.</td>\n",
       "      <td>참고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Replication Data for: When Heads of Government...</td>\n",
       "      <td>Despite representing a crucial day-to-day dipl...</td>\n",
       "      <td>https://doi.org/10.7910/DVN/87YG4K</td>\n",
       "      <td>0.3480</td>\n",
       "      <td>주요 키워드가 부분적으로 유사합니다.</td>\n",
       "      <td>참고</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                                              title  \\\n",
       "0     1  Replication Data for: Placebo-Augmented PICA D...   \n",
       "1     2                                        Local Fairs   \n",
       "2     3                      Escolar - Annual Distribution   \n",
       "3     4  Action to beat Coronavirus Study [Ab-C, study ...   \n",
       "4     5  Replication Data for: When Heads of Government...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Most media experiments analyze the impact of m...   \n",
       "1  Supported by funding from the Department of Ar...   \n",
       "2  <a href='https://maps.mpi.govt.nz/templates/MP...   \n",
       "3  <b>Background:</b> In the early months of the ...   \n",
       "4  Despite representing a crucial day-to-day dipl...   \n",
       "\n",
       "                                                 url   score  \\\n",
       "0                 https://doi.org/10.7910/DVN/1KQN5U  0.4522   \n",
       "1     https://dx.doi.org/10.7925/drs1.duchas_5191737  0.3778   \n",
       "2  https://catalogue.data.govt.nz/dataset/b6cd615...  0.3607   \n",
       "3              https://dx.doi.org/10.5683/SP3/LA2IKO  0.3524   \n",
       "4                 https://doi.org/10.7910/DVN/87YG4K  0.3480   \n",
       "\n",
       "                 reason level  \n",
       "0  주요 키워드가 부분적으로 유사합니다.    강추  \n",
       "1  주요 키워드가 부분적으로 유사합니다.    참고  \n",
       "2  주요 키워드가 부분적으로 유사합니다.    참고  \n",
       "3  주요 키워드가 부분적으로 유사합니다.    참고  \n",
       "4  주요 키워드가 부분적으로 유사합니다.    참고  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 저장 완료:\n",
      " - C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\recommendations.papers.csv\n",
      " - C:\\Users\\ogod3\\바탕 화면\\유섭 - desktop\\취준&대학원\\DATA&AI 공모전\\recommendations.datasets.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== 질의 입력 =====\n",
    "print(\"=== 질의 입력 ===\")\n",
    "USER_TITLE = input(\"제목을 입력하세요: \").strip()\n",
    "USER_DESC  = input(\"설명을 입력하세요(빈칸 가능): \").strip()\n",
    "if not USER_TITLE and not USER_DESC:\n",
    "    raise ValueError(\"제목/설명 중 하나는 반드시 입력해야 합니다.\")\n",
    "\n",
    "# ===== 백엔드 1회 생성 (SBERT/TF-IDF) =====\n",
    "backend = get_backend()\n",
    "\n",
    "# ===== 논문 코퍼스 =====\n",
    "papers_df = load_df(PAPERS_CSV)\n",
    "papers_top = run_recommendation_for_corpus(papers_df, backend, USER_TITLE, USER_DESC, TOPK)\n",
    "papers_top.to_csv(OUTPUT_PAPERS, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ===== 데이터셋 코퍼스 =====\n",
    "datasets_df = load_df(DATASETS_CSV)\n",
    "datasets_top = run_recommendation_for_corpus(datasets_df, backend, USER_TITLE, USER_DESC, TOPK)\n",
    "datasets_top.to_csv(OUTPUT_DATA, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ===== 표시 =====\n",
    "from IPython.display import display\n",
    "print(\"\\n[논문 추천 TOP5]\")\n",
    "display(papers_top)\n",
    "\n",
    "print(\"\\n[데이터셋 추천 TOP5]\")\n",
    "display(datasets_top)\n",
    "\n",
    "import os\n",
    "print(f\"\\n 저장 완료:\\n - {os.path.abspath(OUTPUT_PAPERS)}\\n - {os.path.abspath(OUTPUT_DATA)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e68299-82dc-4acc-9731-7439579055aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
