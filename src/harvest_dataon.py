import requests
import orjson
import time
import os
from tqdm import tqdm
import string
import itertools
import glob

# âœ… [1] API ì¸ì¦ ê´€ë ¨ ì„¤ì •
SEARCH_KEY = "5494BC49983EF849F14BD95428E97132"  # KISTI DataON API key
SEARCH_URL = "http://dataon.kisti.re.kr/rest/api/search/dataset"  # ê²€ìƒ‰ endpoint

# âœ… [2] ì €ì¥ ê´€ë ¨ íŒŒë¼ë¯¸í„°
SPLIT_SIZE = 100_000  # íŒŒì¼ í•˜ë‚˜ë‹¹ ì €ì¥í•  ìµœëŒ€ ë°ì´í„° ìˆ˜ (ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šê²Œ ë¶„í• )
STEP = 100             # í•œ ë²ˆì˜ ìš”ì²­ë‹¹ ê°€ì ¸ì˜¬ ë°ì´í„° ê°œìˆ˜ (API size íŒŒë¼ë¯¸í„°)

# âœ… [3] ì¿¼ë¦¬ ë¬¸ì ì¡°í•© ìƒì„± (2ê¸€ìì”©)
# DataON ê²€ìƒ‰ APIëŠ” ê²€ìƒ‰ì–´(query)ê°€ ìˆì–´ì•¼ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ,
# ê°€ëŠ¥í•œ ëª¨ë“  ë¬¸ì ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰ì„ ì‹œë„í•˜ì—¬ ì „ì²´ ë°ì´í„°ë¥¼ ì»¤ë²„í•¨.

# ì¶”ê°€ë¡œ í¬í•¨í•  íŠ¹ìˆ˜ë¬¸ì ë° ë‹¤êµ­ì–´ ë¬¸ì (ì¼ë³¸ì–´Â·ì¤‘êµ­ì–´Â·í•œê¸€ í˜¼í•©)
EXTRA_CHARS = list("æ—¥æœ¬ä¸­å›½ãƒ‡ãƒ¼ã‚¿æ•°æ®í•™ìŠµì—°êµ¬ê³¼í•™ê¸°ìˆ ")

# ê¸°ë³¸ ë¬¸ì ì§‘í•©: ì˜ì–´ ëŒ€ì†Œë¬¸ì + ìˆ«ì + í•œê¸€ ì¼ë¶€ + íŠ¹ìˆ˜ë¬¸ì
BASE_CHARS = (
    list(string.ascii_lowercase) +  # a-z
    list(string.ascii_uppercase) +  # A-Z
    list(string.digits) +           # 0-9
    list("ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜") +  # ì£¼ìš” í•œê¸€ ì´ˆì„±
    EXTRA_CHARS +                   # ë‹¤êµ­ì–´ ë¬¸ì
    ["-", "_", ".", "@", "#", "&"]  # ìì£¼ ë“±ì¥í•˜ëŠ” íŠ¹ìˆ˜ë¬¸ì
)

# itertools.productì„ ì´ìš©í•´ ëª¨ë“  ê°€ëŠ¥í•œ 2ê¸€ì ì¡°í•© ìƒì„±
# ì˜ˆ: "aa", "ab", "ac", ..., "ê°€a", "A1" ë“±
QUERIES = ["".join(p) for p in itertools.product(BASE_CHARS, repeat=2)]
print(f"[INFO] ì´ {len(QUERIES)} ê°œ ì¿¼ë¦¬ ìƒì„±ë¨")

# âœ… [4] ë°ì´í„° ì €ì¥ ë””ë ‰í„°ë¦¬
SAVE_DIR = "dataon_dumps"
os.makedirs(SAVE_DIR, exist_ok=True)


# =======================
# ğŸ” [5] API ìš”ì²­ í•¨ìˆ˜
# =======================
def fetch_search(query="", start=0, size=100):
    """DataON APIì—ì„œ query ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ë°˜í™˜"""
    params = {"key": SEARCH_KEY, "query": query, "from": start, "size": size}
    r = requests.get(SEARCH_URL, params=params, timeout=20)
    r.raise_for_status()  # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
    return r.json()


# =======================
# ğŸ’¾ [6] JSONL ì €ì¥ í•¨ìˆ˜
# =======================
def save_jsonl(records, file_path):
    """ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë ˆì½”ë“œë¥¼ JSON Lines í¬ë§·ìœ¼ë¡œ íŒŒì¼ì— append"""
    with open(file_path, "ab") as f:
        for rec in records:
            f.write(orjson.dumps(rec, option=orjson.OPT_APPEND_NEWLINE))


# =======================
# ğŸ§  [7] ì¤‘ë³µ ë°©ì§€ìš© ID ë¡œë“œ
# =======================
def load_seen_ids():
    """
    ì´ë¯¸ ì €ì¥ëœ íŒŒì¼(datasets_part*.jsonl)ì—ì„œ 'id' í•„ë“œë¥¼ ì½ì–´ ì¤‘ë³µ ë°©ì§€ìš© Set ìƒì„±
    â†’ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨ í›„ ì¬ì‹¤í–‰ ì‹œ, ì´ì „ì— ì €ì¥í•œ ë°ì´í„°ëŠ” ê±´ë„ˆëœ€
    """
    seen = set()
    files = glob.glob(os.path.join(SAVE_DIR, "datasets_part*.jsonl"))
    for file in files:
        with open(file, "rb") as f:
            for line in f:
                try:
                    obj = orjson.loads(line)
                    if "id" in obj:
                        seen.add(obj["id"])
                except Exception:
                    continue
    print(f"[INFO] ê¸°ì¡´ íŒŒì¼ì—ì„œ {len(seen)} ê°œ svc_id ë¡œë“œë¨")
    return seen


# =======================
# ğŸš€ [8] ë©”ì¸ ë¡œì§
# =======================
def main():
    # ê¸°ì¡´ì— ì €ì¥ëœ ë°ì´í„° ID ë¶ˆëŸ¬ì˜¤ê¸°
    seen_ids = load_seen_ids()
    total_saved = len(seen_ids)  # ì´ë¯¸ ì €ì¥ëœ ë°ì´í„° ìˆ˜

    # í˜„ì¬ íŒŒì¼ ë²ˆí˜¸ ê³„ì‚° (ex: datasets_part1.jsonl, datasets_part2.jsonl)
    part_num = (total_saved // SPLIT_SIZE) + 1
    current_file = os.path.join(SAVE_DIR, f"datasets_part{part_num}.jsonl")

    # ìƒì„±ëœ ëª¨ë“  2ê¸€ì ì¿¼ë¦¬ë¥¼ ìˆœíšŒí•˜ë©° ìˆ˜ì§‘
    for query in QUERIES:
        print(f"\n=== Query: {query} ===")
        try:
            # ì „ì²´ ê±´ìˆ˜ í™•ì¸ (total count)
            first = fetch_search(query=query, start=0, size=1)
            total = first.get("response", {}).get("total count", 0)
            print(f"  â–¶ {query} ê²€ìƒ‰ â†’ {total} ê±´ ë°œê²¬")
        except Exception as e:
            print(f"[!] {query} ê²€ìƒ‰ total count í™•ì¸ ì‹¤íŒ¨:", e)
            continue

        # STEP ë‹¨ìœ„ë¡œ í˜ì´ì§€ë„¤ì´ì…˜í•˜ë©° ë°ì´í„° ìˆ˜ì§‘
        for start in tqdm(range(0, total, STEP), desc=f"{query} ê²€ìƒ‰"):
            try:
                result = fetch_search(query=query, start=start, size=STEP)
            except Exception as e:
                print(f"[!] API ì˜¤ë¥˜ at query={query}, start={start}:", e)
                time.sleep(3)
                continue

            items = result.get("records", [])
            if not items:
                break

            detailed_records = []
            for it in items:
                dataset_id = it.get("svc_id")
                # ì¤‘ë³µ ì œê±°
                if not dataset_id or dataset_id in seen_ids:
                    continue
                seen_ids.add(dataset_id)

                # í•„ìš”í•œ í•„ë“œë§Œ ì •ë¦¬í•˜ì—¬ ì €ì¥
                record = {
                    "id": dataset_id,
                    "title": it.get("dataset_title_kor") or it.get("dataset_title_etc_main"),
                    "description": it.get("dataset_expl_kor") or it.get("dataset_expl_etc_main"),
                    "keywords": [it.get("dataset_kywd_kor"), it.get("dataset_kywd_etc_main")],
                    "org": it.get("cltfm_kor") or it.get("cltfm_etc"),
                    "year": it.get("dataset_pub_dt_pc"),
                    "url": it.get("dataset_lndgpg"),
                    "doi": it.get("dataset_doi"),
                }
                detailed_records.append(record)

            # íŒŒì¼ í¬ê¸°ê°€ SPLIT_SIZEë¥¼ ë„˜ìœ¼ë©´ ë‹¤ìŒ íŒŒì¼ë¡œ ì „í™˜
            if total_saved >= part_num * SPLIT_SIZE:
                part_num += 1
                current_file = os.path.join(SAVE_DIR, f"datasets_part{part_num}.jsonl")
                print(f"\n[INFO] ìƒˆë¡œìš´ íŒŒì¼ ì‹œì‘ â†’ {current_file}")

            # ì €ì¥
            save_jsonl(detailed_records, current_file)
            total_saved += len(detailed_records)

            # ì§„í–‰ìƒí™© ì£¼ê¸°ì ìœ¼ë¡œ ì¶œë ¥
            if start % 10_000 == 0:
                print(f"[INFO] {query} - {start} ~ {start+STEP} ì €ì¥ (ëˆ„ì  {total_saved})")

            # API ê³¼ë¶€í•˜ ë°©ì§€ìš© ëŒ€ê¸° ì‹œê°„
            time.sleep(0.2)

    print(f"\n[DONE] ì´ {total_saved} ê±´ ë°ì´í„° ì €ì¥ ì™„ë£Œ")


# âœ… í”„ë¡œê·¸ë¨ ì§„ì…ì 
if __name__ == "__main__":
    main()